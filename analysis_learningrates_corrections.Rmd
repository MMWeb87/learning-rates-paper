---
title: "Learning rates paper"
editor_options:
  chunk_output_type: inline
output:
  html_document:
    df_print: paged
    toc_depth: '2'
  html_notebook: default
  pdf_document:
    highlight: zenburn
    keep_tex: no
    number_sections: yes
    toc_depth: 2
params:
  always_use_fixed_er: yes
  capacity_threshold: 5
  debug: yes
  delta: BNEF
  exchangerate_reference_month: 1
  lead_currency: USD
  use_kable: no
subtitle: Preliminary results 2 - Not for publication!
bibliography: bib_lc.bib
---

```{r setup, include=FALSE}

library(knitr)
library(ggsci)
#library(ggplot2)
#library(plyr)
#library(dplyr)
#library(reshape2) # melt function
library(gridExtra)
library(tidyverse)
library(lubridate) # year function


knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

```


```{r parameters, include=FALSE}

# to accept these relevant currencies, they need to be in the currency_translation table
relevant_currencies <- c("CNY","EUR","GBP","INR","JPY","USD")

plot_order_currency <- relevant_currencies
plot_order_country <- c("China", "Euro Area", "United Kingdom", "India", "Japan", "United States", "ROW")
plot_order_linetype <- c("dotdash", "longdash", "dashed", "dotted", "twodash", "solid")


intervals <- list(
  c(2006,2011),
  c(2012,2016),
  c(2006,2016)
)

fig.width.baseline <- 7
out.width.default <- "100%"


combined_plot_interval <- "2006-2016"

filenames <- list(
  exchange_rates = "input/USD_exchange_rates.csv"
)


```

```{r Init, echo=FALSE, include=FALSE}


#knitr::opts_chunk$set(fig.pos = 'h')
theme_set(theme_bw(base_size = 9.5) + theme(legend.title=element_blank()))

source("functions_learning.R")


# T0 is the start year and T_max the end year of the interval
# lead currency is the currency of the learning rate
T0 <- min(unlist(intervals))
T_max <- max(unlist(intervals))

lead_currency <- params$lead_currency

#n_relevant_currencies <- length(relevant_currencies)
currencies_print <- paste(relevant_currencies, collapse=", ")
all_subset_intervals <- list(
  i0 = c(paste0(T0,"-01-01"), paste0(T_max,"-01-02"))
)



names(intervals) <- make_interval_names(intervals)


#length(intervals) <- length(intervals)
#n_relevant_currencies <- length(relevant_currencies)
  
```
Date: \today

# Results for PV -- in lead currency `r lead_currency` & with `r params$delta` marketshare 

Here, we present an improved approach that uses global market shares (in this document based on data from `r params$delta`) and a correction factor to calculate a global learning rate that is the same over all currencies. We chose `r length(relevant_currencies)` relevant currencies (`r currencies_print`) and calculate a learning rate between `r T0` and `r T_max`. We look only at PV projects above `r params$capacity_threshold` MW.

# Data

## Load and prepare BNEF data

Our analysis is based on the Bloomberg New Energy Finance (BNEF) renewable energy database. The dataset is copyright protected and can not be shared. We use the BNEF dataset because it gives investment costs for individual installations and projects in the original currencies, or allows for conversion to the domestic currency of each project. We subset the data according to different criteria, as seen below.

```{r Read BNEF dataset, echo=TRUE}

data_projects <- read_csv(
    file = "input/data_PV_all_nominal.csv", 
    col_types = cols(
      `Financing Date` = col_date(format="%Y-%m-%d")),
    na = c("","NA")) %>%
  select(
    id = `Renewable Project ID`,
    capacity = `Capacity - total (MWe)`,
    total_local_value = `Total Value (Local)`,
    local_currency_long = `Local Currency`,
    country = Country,
    date = `Financing Date`,
    status = Status)

commissioned_projects <- filter(
    data_projects, 
    status == "Commissioned")

relevant_projects <- filter(
    commissioned_projects,
    capacity >= params$capacity_threshold,
    !is.na(date))

non_relevant_projects1 <- filter(
    commissioned_projects,
    capacity < params$capacity_threshold)

non_relevant_projects2 <- filter(
    commissioned_projects,
    capacity >= params$capacity_threshold,
    is.na(date))

```


The BNEF dataset does not consistenly report the original, local currencies. In cases where the local currency is missing, we assumed that projects were financed in the main currency of the corresponding project countries. In addition, I transform from total local value in Millions to local value in currency/W.

In addition, we consistently use the short ISO currency codes. Hence we convert the long currency codes to short ones.

```{r Fill missing currency records in BNEF}

# I map short to long currency codes because it's easier to read. 
long_to_short_currencycode_mappingtable <- read_csv("input/currency_translation.csv") %>%
  select(local_currency_short = short, local_currency_long = long)
country_to_currencycode_mappingtable <- read_csv("input/country_to_currency_translation.csv")

relevant_projects_completed <- relevant_projects %>% 
  left_join(
    long_to_short_currencycode_mappingtable, 
    by = "local_currency_long") %>% 
  left_join(
    select(country_to_currencycode_mappingtable, country = country_long, local_currency_added = currency), 
    by = "country") %>% 
  mutate(
    local_currency_final = ifelse(is.na(local_currency_short), local_currency_added, local_currency_short),
    local_value = total_local_value / capacity, # from million currency and MW to currency/W
    year = year(date))

# In these projects, I decided to keep to currency that was originally reported to be more accurate.
relevant_projects_completed %>% filter(local_currency_short != local_currency_added)
  

# To simplify thing, I change the naming here for the final subset
projects_with_costs_with_outliers <- relevant_projects_completed %>% 
  select(id, date, capacity, local_currency = local_currency_final, local_value, year) %>% 
  filter(
    !is.na(local_value),
    !is.na(local_currency),
  local_currency != "Other"
  )

projects_with_costs_with_outliers$local_currency <- as.factor(projects_with_costs_with_outliers$local_currency)

projects_with_costs_with_outliers
  
# TODO
currency_to_currency_area_translation <- read_csv("input/currency_to_currency_area_translation.csv")

country_short_to_currency <- data.frame(currency = country_to_currencycode_mappingtable$currency)
rownames(country_short_to_currency) <- country_to_currencycode_mappingtable$country_short
currency_to_currency_area_translation <- select(currency_to_currency_area_translation, currency = "currency", currency_area)

```

From *projects_with_costs_with_outliers*, we exclude projects as outliers because they have implausible costs that deviate very strongly (a factor 10 or more) from the overall trend and have most likely been entered incorrectly in the BNEF database. Compare the outliers.csv for details and see the plots bellow.

```{r Outlier treatment, fig.cap="Overview of BNEF data and outliers.", fig.width=fig.width.baseline}

outliers_table <- read_csv("input/outliers.csv", col_names = c("id", "desc"), skip = 1)

projects_with_costs_with_outliers <- projects_with_costs_with_outliers %>%  
  mutate(outlier = id %in% outliers_table$id)

n_outliers <- sum(projects_with_costs_with_outliers$outlier)

# projects_with_costs
projects_with_costs <- projects_with_costs_with_outliers %>% 
  filter(outlier == FALSE) %>% 
  select(-outlier)

overview_plots_after <- list()
overview_plots_after_boxplot <- list()

for (var in unique(projects_with_costs_with_outliers$local_currency)) {
  
  overview_plots_after[[var]] <- projects_with_costs_with_outliers %>% 
    filter(local_currency == var) %>% 
    ggplot(aes(date, local_value, shape = outlier, col = outlier)) +
      geom_point() +
      scale_shape_manual(values = c(16, 9)) +
      scale_color_manual(values = c("black", "red")) +
      labs(x = "Year", 
           y = paste("Costs in", var),
          subtitle = paste("Projects reported in", var)) +
      theme(legend.position = "none")
}

with(overview_plots_after, grid.arrange(CNY,EUR,INR,JPY,GBP,USD, ncol=3, widths=c(1, 1, 1)))
```

`r n_outliers` project(s) had very high prices and distorted the average. We treated them as outliers and removed them form the dataset (see figure 3).

```{r Statistical number of BNEF}

stats <- list()

# Different BNEF statistics
stats[["number"]][["all_projects"]] <- nrow(data_projects)
stats[["capacity"]][["all_projects"]] <- sum(data_projects$capacity, na.rm = T)

stats[["number"]][["commissioned_projects"]] <- nrow(commissioned_projects)
stats[["capacity"]][["commissioned_projects"]] <- sum(commissioned_projects$capacity, na.rm = T)

stats[["number"]][["relevant_projects"]] <- nrow(relevant_projects)
stats[["capacity"]][["relevant_projects"]] <- sum(relevant_projects$capacity)

stats[["number"]][["non_relevant_projects1"]] <- nrow(non_relevant_projects1)
stats[["capacity"]][["non_relevant_projects1"]] <- sum(non_relevant_projects1$capacity)

stats[["number"]][["non_relevant_projects2"]] <- nrow(non_relevant_projects2)
stats[["capacity"]][["non_relevant_projects2"]] <- sum(non_relevant_projects2$capacity)

stats[["number"]][["relevant_projects_with_costs_with_outliers"]] <- nrow(projects_with_costs_with_outliers)
stats[["capacity"]][["relevant_projects_with_costs_with_outliers"]] <- sum(projects_with_costs_with_outliers$capacity)

stats[["number"]][["relevant_projects_with_costs"]] <- nrow(projects_with_costs)
stats[["capacity"]][["relevant_projects_with_costs"]] <- sum(projects_with_costs$capacity)

# I delete allbut the projects_with_costs to make sure that I use only projects_with_costs

rm(data_projects, commissioned_projects, relevant_projects, non_relevant_projects1, non_relevant_projects2, projects_with_costs_with_outliers)

# Shares
stats[["number_share"]] <- stats$number/stats[["number"]][["commissioned_projects"]]
stats[["capacity_share"]] <- stats$capacity/stats[["capacity"]][["commissioned_projects"]]


## Overview table
summary_table_rows <- 5
summary_table <- data.frame(
  "description" = character(summary_table_rows),
  "projects" = numeric(summary_table_rows),
  "project share" = numeric(summary_table_rows),
  "capacity" = numeric(summary_table_rows),
  "capacity share" = numeric(summary_table_rows),
  stringsAsFactors = F
)

summary_table[1,] <- list("all", stats[["number"]][["all_projects"]], NA, stats[["capacity"]][["all_projects"]], NA)

summary_table[2,]  <- list("all commissioned", stats[["number"]][["commissioned_projects"]], 1, stats[["capacity"]][["commissioned_projects"]], 1)

summary_table[3,]  <- list("all relevant", 
                           stats[["number"]][["relevant_projects"]],
                           stats[["number_share"]][["relevant_projects"]],
                           stats[["capacity"]][["relevant_projects"]],
                           stats[["capacity_share"]][["relevant_projects"]])

summary_table[4,]  <- list("all relevant with costs and outliers", 
                           stats[["number"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["number_share"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["capacity"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs_with_outliers"]])

summary_table[5,]  <- list("all relevant with costs", 
                           stats[["number"]][["relevant_projects_with_costs"]],
                           stats[["number_share"]][["relevant_projects_with_costs"]],
                           stats[["capacity"]][["relevant_projects_with_costs"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs"]])


kable(summary_table, digits=2, caption = "Summary of dataset. The share relates to the commissioned projects")

```

The BNEF dataset contains a total of `r stats[["number"]][["all_projects"]]` PV projects, of which `r stats[["number"]][["commissioned_projects"]]` are commissioned. Of those commissioned projects, we consider `r stats[["number"]][["relevant_projects"]]` as relevant because they have a capacity of >= `r params$capacity_threshold` MW, which is an indication that their local-cost component is low. Among the comissioned projects, some are not relevant (to caluclate learning rates) because they are either below that threshold (`r stats[["number"]][["non_relevant_projects1"]]`), above but miss a financing date record (`r stats[["number"]][["non_relevant_projects2"]]`) or are reported in other, minor currencies. 

Finally, only **`r stats[["number"]][["relevant_projects_with_costs"]]` projects** have a financing costs in one of the relevant currencies. We use these projects , i.e. `r round(stats[["capacity_share"]][["relevant_projects_with_costs"]]*100,1)`% of worldwide capacity to calculate the learning rates.These numbers and the corresponding capacity they represent are sumarised in the table below.


## Load and prepare IRENA data

We also use the IREAN dataset to calculate market shares, if the paramater is set. The IRENA data better represent the market shares than the BNEF data. For the calculations of the learning rates we need curreny information that the IRENA dataset does not include. Hence, we assume that the projects were financed in the main currency of the corresponding countries.

Because the orignal IRENA dataset

*TODO*: comparison graph.

```{r Load and prepare IRENA data}

# The irena dataset need to have the same format and apply the same criteria as the BNEF dataset (especially capacity threshold)

irena_data <- read_csv("input/irena-capacity-generation.csv") %>% 
  filter(
    Product == "Solar photovoltaic" | Product == "Solar Photovoltaic", 
    Type == "ONG",
    `Capacity (MW)` >= params$capacity_threshold) %>% 
  select(
    capacity_cum = `Capacity (MW)`,
    country = ISO, 
    year = Year) %>%
  arrange(country, year) %>% # need to sort, as I calculate the difference between each year in a loop
  mutate(capacity = capacity_cum) %>% # the simplest way to set the start capacity in each year 
  add_column(local_currency = NA)

irena_data$local_currency <- as.character(irena_data$local_currency)

# change datatypes. Necessary?
# irena_data$year <- as.numeric(irena_data$year)
# irena_data$capacity_cum <- as.numeric(irena_data$capacity_cum)


# We loop through the dataset, because we can both set the irena_data$local_currency and calculate yearly capacity additions instead of the cummulative capacity we have so far.
for(i in 1:nrow(irena_data)){
  
  ith_irena_country <- as.character(irena_data[i,"country"])

  # If the country is in the "country_to_currency_translation" list, it is relevant (rest ROW)
  if(ith_irena_country %in% rownames(country_short_to_currency)){
    irena_data[i,"local_currency"] <- as.character(country_short_to_currency[ith_irena_country,"currency"])
  } else {
    irena_data[i,"local_currency"] <- "Other"
  }

  if (i > 1){

    if(irena_data[i-1,"country"] == irena_data[i,"country"]){
      irena_data[i,"capacity"] <- irena_data[i,"capacity_cum"] - irena_data[i-1,"capacity_cum"]
    }    
  }  
}

rm(i,ith_irena_country)

```



## Exchange rates

Exchange rates are from OFX and OECD, and the defaltors are based on OECD Consumer Price Index (CPI) data.

```{r load exchange rates}

exchange_rates_USD <- read_csv(filenames[["exchange_rates"]], col_types = cols(
    date = col_date(format = "%d.%m.%y"))) %>% 
  drop_na() %>% 
  add_column(USD = 1) %>% 
  select(date, !!relevant_currencies)


exchange_rates_in_lead_currency <- exchange_rates_USD  

if(lead_currency != "USD"){

  for(other_currencies in relevant_currencies){
    # c_curr/c_lead = c_curr/USD (column c_curr in relevant_exchange_rates) * (c_lead/USD)^-1 (column c_lead in relevant_exchange_rates)
  exchange_rates_in_lead_currency[other_currencies] <- exchange_rates_USD[other_currencies] * as_tibble(exchange_rates_USD[lead_currency]^(-1))

  }
}

rm(other_currencies)

# Select the lead currencies exchange rate
exchange_rates <- exchange_rates_in_lead_currency %>% 
  as_tibble() %>% 
  mutate(month = month(date), year = year(date)) %>% 
  select(date, year, month, everything())

exchange_rates_per_year <- exchange_rates %>% 
  filter(month == params$exchangerate_reference_month) %>% 
  select(-c(date))


```

## Deflators


```{r calculate delfators}

## Deflators
# load yearly deflators
cpi_deflators <- read.csv("input/OECD_CPI_yearly.csv", stringsAsFactors = F)
cpi_deflators <- select(cpi_deflators, country = LOCATION, year = TIME, value = Value)
cpi_deflators$year <- as.numeric(cpi_deflators$year)

# change the values to the reference year (2017)
cpi_deflators_index_year <- select(filter(cpi_deflators, year == 2017), country, value_index = value)
cpi_deflators <- merge(cpi_deflators, cpi_deflators_index_year)

# add new variables (currency and indexed value)
cpi_deflators <- mutate(
  cpi_deflators, 
  currency = sapply(country, convert_location_to_currency),
  defl = value / value_index)

# prepare final deflator object
defl <- select(cpi_deflators, year, currency, defl)
defl <- filter(defl, !is.na(currency))
defl$currency <- as.factor(defl$currency)

rm(cpi_deflators_index_year, cpi_deflators)
```

## Cummulative sums

X is the global cumulative deployment in year t -> cummulative sum of x_global (sum of xj) per year
I always use the cummulative capacity from BNEF because it relates to the average costs and projects.


```{r Calculate cummulative sums}

x_global_BNEF <- projects_with_costs %>% 
  group_by(year) %>% 
  summarise(x_global = sum(capacity))

x_global_IRENA <- irena_data %>% 
  group_by(year) %>% 
  summarise(x_global = sum(capacity))

X <- tibble(
  year = x_global_BNEF$year,
  X = cumsum(x_global_BNEF$x_global)
)

```

# Learning rate calculations

## Method 1: Simple learning rates in different currencies 
In Table 1 & 2 and Figure 1, we present the results of the simple learning rate approach.

We calculate the simple learning rate in multiple steps. 

### Step 1: Calculation of costs in every relevant currency

1. we add columns for every relevant currency to the data.frame. 
2. For each project, we calculate the value of the lead currency (default USD) from the local value and an exchange rate that is close to the project date. 
3. We caluculate the values in each relevant currency from the (in step 2 calculates) lead currency value.

```{r Calculate simple learning rates}

projects_with_costs_in_currencies <- projects_with_costs

# add columns for each currency
projects_with_costs_in_currencies[relevant_currencies] <- NA

# finds the closest date in exchange_rates$date vector and returns
get_closest_exchange_rates <- function(project_date, currency) {
  
# exchange_rates is global
  
  if(params$always_use_fixed_er){
    
    er <- exchange_rates_per_year %>% 
      filter(year == year(project_date)) %>% 
      pull(currency)

  } else {
    
    day_differences <- abs(exchange_rates$date - project_date)
    er_date <- which(abs(day_differences) == min(day_differences, na.rm=TRUE))[1] 
    er <- as.double(exchange_rates[er_date,currency])
  }
  return(er)
}

for(j in 1:nrow(projects_with_costs_in_currencies)){
  
  project_date_j = projects_with_costs_in_currencies[[j,"date"]]
  local_currency_j = as.character(projects_with_costs_in_currencies[[j,"local_currency"]])

  projects_with_costs_in_currencies[[j, lead_currency]] <- #
    projects_with_costs_in_currencies[[j,"local_value"]] * get_closest_exchange_rates(project_date_j, local_currency_j)
  
  
  for(relevant_currency in relevant_currencies){
    if(relevant_currency!=lead_currency){
      projects_with_costs_in_currencies[[j, relevant_currency]] <- #
        projects_with_costs_in_currencies[[j, lead_currency]] / get_closest_exchange_rates(project_date_j, relevant_currency)
    }
  }
}

rm(j, project_date_j, local_currency_j)

projects_with_costs_in_currencies


```


### Step 2. Aggregate data to means

We no aggregate the data by yearly averages.
average_global_costs are the aggregated means of all(!) projects (no subset)

```{r aggregate to average costs}

average_global_costs <- projects_with_costs_in_currencies %>% 
  select(year, !!relevant_currencies) %>% 
  gather("currency", "value", !!relevant_currencies) %>% 
  group_by(currency, year) %>% 
  summarise(global_means = mean(value)) %>% ungroup()

```


### Step 3. Convert to real values (in 2017 units)

```{r}

average_global_costs_real <- average_global_costs %>% 
  inner_join(defl, by = c("currency", "year")) %>% 
  mutate(average_costs = global_means / defl)

```

### Step 4. Calculation of learning rates

```{r Calculation of learning rates}

simple_learning_rates_matrix <- matrix(
  ncol = length(intervals), 
  nrow = length(relevant_currencies))

simple_learning_rates_rsquared_matrix <- simple_learning_rates_matrix

for(j in seq_along(relevant_currencies)){
  currency <- relevant_currencies[[j]]

  for(i in seq_along(intervals)){
    interval <- intervals[[i]]
    interval_name <- names(intervals[i])
    interval_years <- as.numeric(interval[1]):as.numeric(interval[2])
    
    regression_costs <- average_global_costs_real %>% 
      filter(year %in% !!interval_years, currency == !!currency) %>% 
      pull(average_costs)
    
    regression_cumulative_capacity <- X %>% 
      filter(year %in% interval_years) %>% 
      pull(X)
    
    if(params$debug){ print(paste("Calc LR with simple methods in currency ", currency, "in interval", interval_name)) }
    
    l <- calculate_learning_rate(regression_costs, regression_cumulative_capacity)
    
    simple_learning_rates_matrix[j,i] <- l$learning_rate
    simple_learning_rates_rsquared_matrix[j,i] <- l$rsquared
  }
}

rm(currency)
  
```


```{r Learning rate bar graph and table, fig.cap="Simple learning rates in different intervals.", fig.width=fig.width.baseline * 0.7, fig.asp= 0.7}

simple_learning_rates <- data.frame(simple_learning_rates_matrix)
rownames(simple_learning_rates) <- relevant_currencies
colnames(simple_learning_rates) <- names(intervals)

simple_learning_rates_rsquared <- data.frame(simple_learning_rates_rsquared_matrix)
rownames(simple_learning_rates_rsquared) <- relevant_currencies
colnames(simple_learning_rates_rsquared) <- names(intervals)

simple_learning_rates_sd <- sapply(simple_learning_rates, sd)

if(params$use_kable){
  kable(rbind(simple_learning_rates,"sd"=simple_learning_rates_sd), digits=2, caption = "Simple learning rates in all currencies. The SD describes the variation between the different learning rates.")
  kable(simple_learning_rates_rsquared, digits=2, caption = "R-squared values of simple learning rates")
} else {
  simple_learning_rates
  simple_learning_rates_rsquared
}


selected_simple_learning_rates <- simple_learning_rates[,c("2006-2011","2012-2016","2006-2016")]

selected_simple_learning_rates$currency <- as.factor(rownames(selected_simple_learning_rates))
 
selected_simple_learning_rates %>% 
  gather(key = interval, value = lr, contains("-")) %>% 
  ggplot(aes(fill=currency, y=lr, x=interval)) + 
    geom_bar(position="dodge", stat="identity") +
    labs(x = "", 
       y = paste0("Learning rate")) +
    guides(fill=guide_legend(title="Currency")) +   
    scale_fill_npg()


```

\clearpage


## Method 2:  Currency-corrected learning rates

A detailed explanation of the theory behind the currency-corrected learning rates is in Bjarnes note.

### Step 1: Marketshare calculation
In this method, the marketshares are not included in the global means. Instead, they are extracted first. 
In this section, I use two types of market share calculation. Once from the orignal data and from IRENA. The IRENA data represent global market shares more accurately.

```{r Calculation of market shares}

## Calculate the deltas and use the corresponding global deplyment figures
delta_BNEF <- calculate_delta(projects_with_costs, x_global_BNEF)
delta_IRENA <- calculate_delta(irena_data, x_global_IRENA)


# Select the correct delta
if(params$delta == "IRENA"){
  delta <- delta_IRENA
} else {
  delta <- delta_BNEF
}

delta$currency <- factor(delta$currency, levels = c(relevant_currencies, "Other"))

## Overview and plot

all_years <- as.numeric(T0):as.numeric(T_max)
delta_print <- delta[order(delta$year) & delta$year %in% all_years,] 
delta_print <- merge(currency_to_currency_area_translation,delta_print)

delta_graph <- delta_print
delta_graph$currency_area <- as.factor(delta_graph$currency_area)
delta_graph$currency_area <- factor(delta_graph$currency_area, plot_order_country)


colnames(delta_print) <- c("original currency", "currency area", "year","deployment in currency (x)", "sum of deployments", "share (delta)") 


delta_plot1 <- ggplot(data=delta_graph, aes(x=year, y=x, fill=currency_area)) +
  geom_bar( stat='identity') +
  scale_x_continuous(breaks = seq(2004,2017,1), minor_breaks = NULL) +
  scale_fill_npg() +
  labs(x = "", 
       y = paste0("Added renewable capacity [MW]")) +
  guides(fill=guide_legend(title="Currency area")) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust = 0.5 ))

delta_plot2 <- ggplot(data=delta_graph, aes(x=year, y=delta, fill=currency_area)) +
  geom_bar(position = "fill", stat='identity') +
  scale_x_continuous(breaks = seq(2004,2017,1), minor_breaks = NULL) +
  scale_fill_npg() +
  labs(x = "", 
       y = paste0("Global share of added renewable capacity")) +
  guides(fill=guide_legend(title="Currency area")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust = 0.5), legend.position="none")

delta_plot_legend <- get_legend(delta_plot1)
delta_plot1 <- delta_plot1 + theme(legend.position="none")
delta_plot1
```


#### Marketshare plots (`r params$delta`)

The market share of PV deplyoment in each currency and year is relevant. To calculate the market share of different currencies, we derive delta values from BNEF and IRENA datasets. Figure 1 visualises the market shares as a function of the currency.

```{r Marketshare plots, fig.width = 11, fig.cap = "Capacity additions per year and currency area", fig.width=fig.width.baseline, fig.asp=0.45, out.width=out.width.default}


grid.arrange(delta_plot1,delta_plot2, delta_plot_legend, ncol=3, widths=c(2.2, 2.2, 0.9))

print_number_table(delta_print, "Shares of deployment")






```

The following table shows the share of all PV capacity between 2006 and 2016

```{r}

#kable(delta_print_all, digits=3, caption = "Shares of deployment", row.names = F)

```




### Step 2: Calculation of the formula variable

C are observed average cost in currency i. We use the variable average_local_costs

```{r Calculation of average_local_costs or C}

## C = average_local_costs: observed average cost per MW in currency i in year t (i.e. not a subset of all projects) 

average_local_costs <- projects_with_costs %>% 
  group_by(year, local_currency) %>% 
  summarise(C = mean(local_value)) %>% ungroup() %>%
  rename(currency = local_currency) %>% 
  complete(year,currency, fill = list(C = 0))

#C$year <- as.character(C$year)
#C$currency <- as.character(C$currency)

table(average_local_costs$year,average_local_costs$currency)

```

The table shows which currency have average_local_costs in which year. 

### Step 3: Calculation of the weighted average global

Because there is exchange rate data for each month, I select a month based on the exchangerate_reference_month parameter 


```{r Calculation of the weighted average global cost P}

## w: exchange rate between currency i and the lead currency l in price notation
# e.g. "5 lead-dollars / 1 Euro" 

# I changed to simple variable name 

C <- average_local_costs

w <- exchange_rates_per_year %>% 
  gather(one_of(relevant_currencies), key = "currency", value = "w") %>% 
  select(-month) %>% 
  add_column(l = lead_currency)

#w$year <- as.character(w$year)
#w$currency <- as.character(w$currency)
#w$w <- as.numeric(w$w)

P <- tibble(
  "year" = all_years,
  "P" = numeric(length(all_years))
)

P_tilde <- P
P_real <- P
P_tilde_real <- P

# alpha per years
alphas <- tibble(
  "year" = all_years,
  "alpha" = numeric(length(all_years))
)

# Loop through all years and currencies to calculate P's
for(t in all_years){
  
  for(i in relevant_currencies){

    # variables for currency i = current_currency and year = T_max
    delta_i_t <- filter(delta, year == t, currency == i) %>% pull(delta)
    w_i_t <- filter(w, year == t, currency == i) %>% pull(w)
    w_i_0 <- filter(w, year == T0, currency == i) %>% pull(w)
    C_i_t <- filter(C, year == t, currency == i) %>% pull(C)
    
    # weighted average global cost converted to lead currency l in year t
    P[P$year == t, "P"] <- P[P$year == t, "P"] + delta_i_t * w_i_t * C_i_t
    P_tilde[P_tilde$year == t, "P"] <- P_tilde[P_tilde$year == t, "P"] + delta_i_t * w_i_0 * C_i_t

  } # end currency loop
  
  alphas[alphas$year == t, "alpha"] <- P_tilde[P_tilde$year == t, "P"]  / P[P$year == t, "P"]
  
  # Deflate the average global costs by the respective deflator of a year
  defl_l_t <- subset(defl, year == t & currency == lead_currency)$defl
  P_real[P_real$year == t, "P"] <- P[P$year == t, "P"] / defl_l_t
  P_tilde_real[P_tilde_real$year == t, "P"] <- P_tilde[P_tilde$year == t, "P"] / defl_l_t
  
  
} # end year loop

```



### Step 4: Calculate learning rate

```{r Calculation of unadjusted and adjusted learning rate}


corrected_learning_rates_list <- list()
corrected_learning_rates_data_list <- list()
corrected_learning_rates_rsquared_list <- list()

types <- c("unadjusted","adjusted")

for(type in types){
  corrected_learning_rates_list[[type]] <- list()
  corrected_learning_rates_data_list[[type]] <- list()
  corrected_learning_rates_rsquared_list[[type]] <- list()
  
  for(i in 1:length(intervals)){
    interval <- intervals[[i]]
   
    interval_years <- as.numeric(interval[1]):as.numeric(interval[2])
    interval_name <- names(intervals[i])
    
    if(type == "unadjusted"){
      relevant_P <- P_real
    } else {
      relevant_P <- P_tilde_real
    }
    
    regression_costs <- relevant_P %>% 
      filter(year %in% interval_years) %>% 
      pull(P)
    
    regression_cumulative_capacity <- X %>% 
      filter(year %in% interval_years) %>% 
      pull(X)
    
    if(params$debug){ print(paste("Calc LR with Method 2 and type", type, "in interval", interval_name)) }
    
    l <- calculate_learning_rate(regression_costs, regression_cumulative_capacity)
    
    corrected_learning_rates_list[[type]][[interval_name]] <- l$learning_rate
    corrected_learning_rates_rsquared_list[[type]][[interval_name]] <- l$rsquared
    
    corrected_learning_rates_data_list[[type]][[interval_name]] <- data.frame(
      rsquared = l$rsquared,
      lead_currency = lead_currency,
      years = interval_years,
      capacity = regression_cumulative_capacity,
      costs = regression_costs,
      alpha = alphas[alphas$year %in% interval_years, ],
      type = type,
      interval = interval_name
    )
  
  }
}
```

A regression over $P_{t,2017}^l$ gives us the **un**adjusted global learning rates in `r lead_currency`,. The **un**adjusted rate includes both technological learning and changes in exchange rates. Therefore, it is different in different lead currencies. To get the same number in every lead currency (i.e. a truly global learning rate), we adjust the unadjusted global learning rate for the currency effect. Applying $\alpha$ to the learning rate calculation results in the adjusted global learning rate. The adjusted rate only includes technological learning. The corresponding learning curves are depicted in Figure 1.


```{r Results table}

corrected_learning_rates <- matrix(unlist(corrected_learning_rates_list), ncol = length(intervals), byrow = TRUE)
corrected_learning_rates_rsquared <- matrix(unlist(corrected_learning_rates_rsquared_list), ncol = length(intervals), byrow = TRUE)

colnames(corrected_learning_rates) <- names(intervals)
rownames(corrected_learning_rates) <- types

colnames(corrected_learning_rates_rsquared) <- names(intervals)
rownames(corrected_learning_rates_rsquared) <- types

print_number_table(corrected_learning_rates, paste("Global learning rates in lead currency", lead_currency))
print_number_table(corrected_learning_rates_rsquared, "R-squared of global learning rates")


```


```{r Plot of learning curves, fig.cap="Plot of the currency-corrected learning curve in 3 intervals", fig.width=fig.width.baseline * 0.8, fig.asp=0.6}


data_for_plots <- do.call("rbind", corrected_learning_rates_data_list$adjusted)

ggplot(data_for_plots, aes(capacity/1000, costs)) +
  geom_smooth(mapping = aes(col = interval, linetype = interval), method="lm", formula = (y ~ x), se=FALSE) +
  geom_point(data=. %>% filter(interval==combined_plot_interval), col = "black") +
  geom_text(data=. %>% filter(interval==combined_plot_interval), col = "black", aes(label=years), hjust=1.3, vjust=1, size=2.5, alpha = 0.8) +
  scale_x_continuous(trans="log", breaks = c(c(1,seq(0,10,2)) %o% 10^(0:4)), minor_breaks = seq(0,1,by = 0.5)) +
  scale_y_continuous(trans="log", breaks = c(2:10 %o% 10^(0:4))) +
  labs(x = "Cumulative capacity [GW]", 
       y = paste0("Weighted average costs [2017-", lead_currency, " / W]")) +
  guides(colour=guide_legend(title="Interval"),linetype=guide_legend(title="Interval")) +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1))
 
```



```{r Combined plot, fig.cap="Comparison of the simple (thin lines) and currency-corrected (think line) learning rates.", fig.width=fig.width.baseline * 0.8, fig.asp=0.6}

# adjusted currency corrected average costs
corrected_learning_rates <- select(
  corrected_learning_rates_data_list$adjusted[[combined_plot_interval]],
  year = years, 
  average_costs = costs, 
  currency = lead_currency, 
  cumulative_capacity = capacity,
  type) %>% 
  as_tibble()

# according to method 1
uncorrected_learning_rates <- average_global_costs %>% 
  inner_join(X) %>% 
  select(
    year, 
    average_costs = global_means, 
    currency, 
    cumulative_capacity = X) %>% 
  add_column(type = "simple")

# combine C_global_2017 (simple) and data_adjusted (currency-corrected) to combined_plot_data and filter for relevant years.
combined_plot_data <- bind_rows(corrected_learning_rates, uncorrected_learning_rates) %>% 
  filter(year >= T0, year <= T_max)

combined_plot_data$currency <- factor(combined_plot_data$currency,plot_order_currency) # order

rm(corrected_learning_rates, uncorrected_learning_rates)


## Norm with one random project to make the combined plot. This gives us an index if that one project of 10 MW.
# Set the norm_project

norm_project <- projects_with_costs_in_currencies %>% 
  filter(year == "2010") %>% 
  filter(row_number()==1)
  
norm_projects <- norm_project %>% 
  select(one_of(relevant_currencies)) %>% 
  gather(everything(), key = "currency", value = "norm_value")


combined_plot_data_normed <- combined_plot_data %>% 
  inner_join(norm_projects, by = "currency") %>% 
  mutate(normed_average_costs = average_costs / norm_value) %>% 
  arrange(type, currency, year)

ggplot(combined_plot_data_normed, aes(cumulative_capacity/1000, normed_average_costs)) +
  geom_smooth(data=. %>% filter(type=="simple"), aes(col=currency, linetype = currency), lwd=0.6 ,method="lm", formula = (y ~ x), se=FALSE) +
  geom_smooth(data=. %>% filter(type=="adjusted"), linetype = "solid", lwd=1.3, method="lm", formula = (y ~ x), se=FALSE, col="black") +
  scale_linetype_manual(values=plot_order_linetype) +
  scale_x_continuous(trans="log", breaks = c(c(1,seq(2,10,2)) %o% 10^(0:4)), minor_breaks = 0.5) +
  scale_y_continuous(trans="log", breaks = seq(0,8,0.5), minor_breaks = NULL) +
  labs(x = "Cumulative capacity [GW]", 
       y = paste0("Index of average yearly costs per MW"),
       subtitle = paste("The data is an index of a project from ", norm_project$year," (financed in", norm_project$local_currency, ",capacity of", norm_project$capacity, "MW)")) +
  guides(linetype=guide_legend(title="Learning rate"), col=guide_legend(title="Learning rate"), colour=guide_legend(title="Currency")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_color_npg()



```

\clearpage

# Appendix


## Calculation

### calculation of currency-corrected learning rates

We calculate our improved currency-corrected learning rate in the lead currency **`r lead_currency`** The resulting, single learning rate is based on the *weighted average global cost converted to lead currency* $P_{t,2017}^l$. We derive $P_{t,2017}^l$ by:

1. calculating $C_{t}^{i}$, i.e. the yearly means of all costs reported in currency i (a subset of all costs).
2. weighting the (nominal) average costs $C_{t}^{i}$ in each year $t$ by the market share in year $t$.
3. converting the weighted average costs to the lead currency.
4. taking the sum of all weighted costs in the lead.
5. applying a deflator to get the (real) 2017 costs.

The theoretical input paper by Bjarne describes the calculations in more detail.

### Calculation of Simple learning rates in different currencies 

To calculate the simple learning rates, we convert **all available project costs** to different currencies. We then derive the learning rate for each currency from the global average costs $C_{global,t,2017}^{i}$ of all available projects. We derive $C_{global,t,2017}^{i}$ for each year $t$ and currency $i$ by 

1. converting all project costs (N = `r stats[["number"]][["relevant_projects_with_costs"]]`) to currency $i$,
2. aggretating these costs (in currency $i$) to yearly means and by
3. applying a deflator to get the (real) 2017 costs.

## Data

### Exchangerates and deflators



Exchange rates are from OFX and OECD, and the defaltors are based on OECD Consumer Price Index (CPI) data.

```{r Exchangerate plot, eval=FALSE, fig.asp=0.6, fig.cap="Comparison of all different exchange rates", fig.width=fig.width.baseline * 0.8, include=FALSE}

# convert exchange rate df to matrix and apply a vector multiplication
exchange_rates_matrix <- as.matrix(exchange_rates[, -1])
exchange_rates_norm_vector <- as.numeric(exchange_rates[241, -1])
exchange_rates_matrix_norm <- sweep(exchange_rates_matrix, 2, exchange_rates_norm_vector, `/`)


exchange_rates_index <- cbind(exchange_rates$date, as.data.frame(exchange_rates_matrix_norm))
names(exchange_rates_index)[1] <- "date"
exchange_rates_index$date <- as.Date(exchange_rates_index$date)

# Only above certain date
exchange_rates_index <- filter(
  exchange_rates_index,
  date >= as.Date("2006-01-01")
)

exchange_rates_index <- melt(exchange_rates_index, id.vars = "date", variable.name = "currency")

exchange_rates_index$currency <- as.factor(exchange_rates_index$currency)
exchange_rates_index$currency <- factor(exchange_rates_index$currency, plot_order_currency)

ggplot(exchange_rates_index, aes(x = date, y = value, col = currency, linetype = currency)) + 
  geom_line() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_linetype_manual(values=plot_order_linetype) +
  scale_color_npg() +
  labs(x = "", 
       y = paste("Index of", lead_currency ," exchange rate"),
      subtitle = "The data is an index of an exchange rate in 2010") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust = 0.5 ))





```

```{r Deflator plot, eval=FALSE, fig.asp=0.6, fig.cap="Deflation", fig.width=fig.width.baseline * 0.8, include=FALSE}
defl_plot <- subset(defl, year>=T0, year<=T_max)
defl_plot$year <- ymd(paste(defl_plot$year,"-01-01"))

ggplot(defl_plot, aes(year, defl, col = currency, linetype = currency)) + 
  geom_line() + 
  scale_linetype_manual(values=plot_order_linetype) +
  scale_color_npg() +
  labs(
    x = "Year",
    y = "Deflation relative to 2017",
    subtitle = "Deflators are based on CPIs")

```




## Handling of missing currency data in the datasets

The marketshare (i.e. deltas) are based on the currency information of different projects (e.g. USD, EUR). However, some BNEF and all IRENA projects lacked this information. Instead they were reported on a country-specific basis. To derive the necessary currency information, we applied some data transformations. 

Based on the assumption that all projects reported in country X are financed in the main currency x of X, we complemented the corresponding projects X with currency x. Our assumption is justified by an inital data exploration that shows that this correlation holds true for most projects that had the currency information in the first place.

## Linear Regression and learning rate calculation

To derive the learning curves, we applied a single-factor learning curve as defined in [@Lindman:2012kv]:

$ln(C_t) = \beta_0 + \beta_1 \cdot ln(CC_t).$

with, $C_t$ as the cost at time $t$, $CC_t$ as cummulative capacity at time $t$, and $\beta_0$ and $\beta_1$ as linear regression estimates. The learning rates is then derived from $\beta_1$:

$LR = 1 - 2^{\beta_1}.$

Based on these formulas, we applied a linear regression model with OLS (Ordinary Least Squares) over the transformed data (with ln-ln transformation of the predictors, as visible in the first formula). 

# Literature
