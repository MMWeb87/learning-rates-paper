---
title: "The dependency of learning rates on exchange rate fluctuations and a method to correct them"
subtitle: Sourcefile and calculations
editor_options:
  chunk_output_type: inline
output:
  html_document:
    df_print: paged
    toc_depth: '2'
  html_notebook: default
  pdf_document:
    highlight: zenburn
    keep_tex: yes
    number_sections: yes
    toc_depth: 2
params:
  capacity_data: IRENA
  capacity_threshold: 5
  lead_currency: USD
  exchange_rate_norm_year: 2006
  deflate: yes
  default_digits: 2
  debug: no
  use_kable: no
  run_test: no
---

```{r setup, include=FALSE}

remove(list = ls()[ls() != "params"])

library(knitr)
library(ggsci)
library(gridExtra)
library(tidyverse)
library(lubridate)
library(glue)
library(cowplot)
library(ggrepel)

source("functions_learning.R")
source("init.R")


lead_currency <- params$lead_currency
deflation_text <- if_else(params$deflate, "real costs", "nominal costs")
version <- if_else(params$deflate, "real", "nominal")


# for report
currencies_print <- paste(relevant_currencies, collapse=", ")

knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE,
  fig.width=fig.width.baseline*output.size, 
  out.width=out.width.default)

theme_set(theme_bw(base_size = 9.5) + theme(
  legend.title=element_blank()),
  legend.spacing.x = unit(0.1, 'cm'))

```


# Results for PV -- in lead currency `r params$lead_currency` & with `r params$capacity_data` marketshare in `r deflation_text` 

This file documents the method of the our learning rate paper. In it,  we present an improved approach that uses global market shares (in this document based on data from `r params$capacity_data`) and a correction factor to calculate a global learning rate that is the same over all currencies. 

## Parameters

We chose `r length(relevant_currencies)` relevant currencies (`r currencies_print`) and calculate a learning rate between `r T0` and `r T_max`. We look only at PV projects above `r params$capacity_threshold` MW.

# Data

## Translation tables
These tables convert strings into other strings, mostly used for currency and country code conversion.

```{r translation tables}

# long currency codes to short codes (We consistently use the short ISO currency codes)
long_to_short_currencycode_mappingtable <- read_csv(filenames$long_to_short_currencycode_translation) %>%
  select(local_currency_short = short, local_currency_long = long)

# country short codes to currencies
country_to_currencycode_mappingtable <- read_csv(filenames$country_to_currency_translation)
country_short_to_currency <- data.frame(currency = country_to_currencycode_mappingtable$currency)
rownames(country_short_to_currency) <- country_to_currencycode_mappingtable$country_short

# currency to currency area
currency_to_currency_area_translation <- read_csv(filenames$currency_to_currency_area_translation) %>% 
  select(currency = "currency", currency_area) %>% 
  mutate(currency_area = fct_relevel(currency_area, plot_order_country))

```


## Load and prepare BNEF data

Our analysis is based on the Bloomberg New Energy Finance (BNEF) renewable energy database. The dataset is copyright protected and can not be shared. We use the BNEF dataset because it gives investment costs for individual installations and projects in the original currencies, or allows for conversion to the domestic currency of each project.

As base unit, we use local value per capacity (e.g. USD/W)

```{r Read BNEF dataset}

data_projects <- read_csv(
    file = filenames$bnef_data, 
    col_types = cols(
      `Financing Date` = col_date(format="%Y-%m-%d")),
    na = c("","NA")) %>%
  select(
    id = `Renewable Project ID`,
    capacity = `Capacity - total (MWe)`,
    total_local_value = `Total Value (Local)`,
    total_usd_value = `Total Value ($m)`,
    local_currency_long = `Local Currency`,
    country = Country,
    date = `Financing Date`,
    status = Status) %>% 
  mutate(
    local_value = total_local_value / capacity, # from million currency and MW to currency/W
    year = year(date)
  ) %>% 
  add_column(subset_name = "All projects")


```

We subset the data according to different criteria:
 
```{r subset BNEF dataset, echo=TRUE}

commissioned_projects <- filter(data_projects, 
    status == "Commissioned") %>% 
  mutate(subset_name = "BNEF commissioned")

large_projects <- filter(commissioned_projects,
    capacity >= params$capacity_threshold) %>% 
  mutate(subset_name = "BNEF >= 5MW")

relevant_projects <- filter(large_projects,
    !is.na(date)) %>% 
  mutate(subset_name = "BNEF with date")

projects_with_costs_with_outliers <- relevant_projects %>% 
  left_join(
    long_to_short_currencycode_mappingtable, 
    by = "local_currency_long") %>% 
  select(id, date, capacity, local_currency = local_currency_short, local_value, year) %>% 
  filter(
    !is.na(local_value),
    !is.na(local_currency),
    local_currency != "Other"
  ) %>% 
  mutate(
    subset_name = "BNEF with costs with outliers",
    local_currency = as.factor(local_currency))


```


### Outlier treatment

```{r Outlier treatment, fig.cap="Overview of BNEF data and outliers.", fig.width=fig.width.baseline}

outliers_table <- read_csv(filenames$outliers, col_names = c("id", "desc"), skip = 1)

projects_with_costs_with_outliers <- projects_with_costs_with_outliers %>%  
  mutate(outlier = id %in% outliers_table$id)

n_outliers <- sum(projects_with_costs_with_outliers$outlier)

# projects_with_costs
projects_with_costs <- projects_with_costs_with_outliers %>% 
  filter(outlier == FALSE) %>% 
  select(-outlier) %>% 
  mutate(subset_name = "BNEF with costs")

overview_plots_after <- list()
overview_plots_after_boxplot <- list()

for (var in unique(projects_with_costs_with_outliers$local_currency)) {
  
  overview_plots_after[[var]] <- projects_with_costs_with_outliers %>% 
    filter(local_currency == var) %>% 
    ggplot(aes(date, local_value, shape = outlier, col = outlier)) +
      geom_point() +
      scale_shape_manual(values = c(16, 9)) +
      scale_color_manual(values = c("black", "red")) +
      labs(x = "Year", 
           y = paste("Costs in", var),
          subtitle = paste("Projects reported in", var)) +
      theme(legend.position = "none")
}

with(overview_plots_after, grid.arrange(CNY,EUR,INR,JPY,GBP,USD, ncol=3, widths=c(1, 1, 1)))
```

From *projects_with_costs_with_outliers*, we exclude `r n_outliers` projects highlighted in the plot. Compare the outliers.csv for details and justifications

### Apply interval

```{r}

# Let's work only with the smaller dataset in the relevant we use.
projects_with_costs_in_interval <- filter(projects_with_costs,
  between(year, T0, T_max)) %>% 
  mutate(subset_name = "BNEF with costs")

```


### Overview

```{r Statistical number of BNEF}

stats <- list()

# Different BNEF statistics
stats[["number"]][["all_projects"]] <- nrow(data_projects)
stats[["capacity"]][["all_projects"]] <- sum(data_projects$capacity, na.rm = T)

stats[["number"]][["commissioned_projects"]] <- nrow(commissioned_projects)
stats[["capacity"]][["commissioned_projects"]] <- sum(commissioned_projects$capacity, na.rm = T)

stats[["number"]][["large_projects"]] <- nrow(large_projects)
stats[["capacity"]][["large_projects"]] <- sum(large_projects$capacity)

stats[["number"]][["relevant_projects"]] <- nrow(relevant_projects)
stats[["capacity"]][["relevant_projects"]] <- sum(relevant_projects$capacity)

stats[["number"]][["relevant_projects_with_costs_with_outliers"]] <- nrow(projects_with_costs_with_outliers)
stats[["capacity"]][["relevant_projects_with_costs_with_outliers"]] <- sum(projects_with_costs_with_outliers$capacity)

stats[["number"]][["relevant_projects_with_costs"]] <- nrow(projects_with_costs)
stats[["capacity"]][["relevant_projects_with_costs"]] <- sum(projects_with_costs$capacity)

stats[["number"]][["relevant_projects_with_costs_in_interval"]] <- nrow(projects_with_costs_in_interval)
stats[["capacity"]][["relevant_projects_with_costs_in_interval"]] <- sum(projects_with_costs_in_interval$capacity)


# Shares
stats[["number_share"]] <- stats$number/stats[["number"]][["commissioned_projects"]]
stats[["capacity_share"]] <- stats$capacity/stats[["capacity"]][["commissioned_projects"]]


## Overview table
summary_table_rows <- 5
summary_table <- tibble(
  "Filter criterium" = character(summary_table_rows),
  "Projects" = numeric(summary_table_rows),
  "Share projects" = numeric(summary_table_rows),
  "Capacity" = numeric(summary_table_rows),
  "Share capacity" = numeric(summary_table_rows))

summary_table[1,] <- list("All BNEF projects", stats[["number"]][["all_projects"]], NA, stats[["capacity"]][["all_projects"]], NA)


summary_table[2,]  <- list("Commissioned projects", stats[["number"]][["commissioned_projects"]], 1, stats[["capacity"]][["commissioned_projects"]], 1)

summary_table[3,]  <- list("Capacity >= 5MW", 
                           stats[["number"]][["large_projects"]],
                           stats[["number_share"]][["large_projects"]],
                           stats[["capacity"]][["large_projects"]],
                           stats[["capacity_share"]][["large_projects"]])


summary_table[4,]  <- list("With financing date reported", 
                           stats[["number"]][["relevant_projects"]],
                           stats[["number_share"]][["relevant_projects"]],
                           stats[["capacity"]][["relevant_projects"]],
                           stats[["capacity_share"]][["relevant_projects"]])

summary_table[5,]  <- list("With financing costs reported", 
                           stats[["number"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["number_share"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["capacity"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs_with_outliers"]])

summary_table[6,]  <- list("Without outliers", 
                           stats[["number"]][["relevant_projects_with_costs"]],
                           stats[["number_share"]][["relevant_projects_with_costs"]],
                           stats[["capacity"]][["relevant_projects_with_costs"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs"]])

summary_table[7,]  <- list(paste("Between", names(intervals[3])), 
                           stats[["number"]][["relevant_projects_with_costs_in_interval"]],
                           stats[["number_share"]][["relevant_projects_with_costs_in_interval"]],
                           stats[["capacity"]][["relevant_projects_with_costs_in_interval"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs_in_interval"]])

# Remove all projects for report. In GW.
summary_table_print <- summary_table %>% 
  mutate(Capacity  = Capacity/1000) %>% 
  slice(-1)
  

print_number_table(summary_table_print, caption = "Summary of our subsets of the BNEF dataset. All lower rows include the filter criteria from the upper rows. Capacity in GW.", digits = 2)


```


## Load and prepare IRENA data

We use IRENA dataset to calculate market shares. To get a good estimate of the learning rate, the underyling cummulative capacity should reflect the actual global deplyoment. The IRENA data better represent the market shares than the BNEF data. For the calculations of the learning rates we need curreny information that the IRENA dataset does not include. Hence, we assume that projects in a country were financed in the main currencies. Compare Step 1 of Method 2 for the graph.

```{r Load and prepare IRENA data}

# The irena dataset need to have the same format and apply the same criteria as the BNEF dataset (especially capacity threshold)

irena_data <- read_csv(filenames$irena_data) %>% 
  filter(
    Product == "Solar photovoltaic" | Product == "Solar Photovoltaic", 
    Type == "ONG",
    `Capacity (MW)` >= params$capacity_threshold) %>% 
  select(
    capacity_cum = `Capacity (MW)`,
    country = ISO, 
    year = Year) %>%
  arrange(country, year) %>% # need to sort, as I calculate the difference between each year in a loop
  mutate(capacity = capacity_cum) %>% # the simplest way to set the start capacity in each year 
  add_column(local_currency = NA) %>% 
  add_column(subset_name = "IRENA")

irena_data$local_currency <- as.character(irena_data$local_currency)


# We loop through the dataset, because we can both set the irena_data$local_currency and calculate yearly capacity additions instead of the cummulative capacity we have so far.
for(i in 1:nrow(irena_data)){
  
  ith_irena_country <- as.character(irena_data[i,"country"])

  # If the country is in the "country_to_currency_translation" list, it is relevant (rest ROW)
  if(ith_irena_country %in% rownames(country_short_to_currency)){
    irena_data[i,"local_currency"] <- as.character(country_short_to_currency[ith_irena_country,"currency"])
  } else {
    irena_data[i,"local_currency"] <- "Other"
  }

  if (i > 1){

    if(irena_data[i-1,"country"] == irena_data[i,"country"]){
      irena_data[i,"capacity"] <- irena_data[i,"capacity_cum"] - irena_data[i-1,"capacity_cum"]
    }    
  }  
}

rm(i,ith_irena_country)

```


## Exchange rates

Exchange rates are from OFX and OECD, and the defaltors are based on OECD Consumer Price Index (CPI) data. We take the yearly average as basis for our calculations

```{r load exchange rates}

exchange_rates_USD_per_month <- read_csv(filenames[["exchange_rates"]], col_types = cols(
    date = col_date(format = "%d.%m.%y"))) %>% 
  drop_na() %>% 
  add_column(USD = 1) %>% 
  select(date, !!relevant_currencies)

exchange_rates_USD_yearly_average <- exchange_rates_USD_per_month %>% 
  gather(-date, key = "currency", value = "er") %>% 
  mutate(year = year(date)) %>% 
  group_by(year, currency) %>% 
  summarise(average_er = mean(er)) %>% ungroup() %>% 
  spread(key = currency, value = average_er)

exchange_rates_in_lead_currency_per_year <- exchange_rates_USD_yearly_average  
exchange_rates_in_lead_currency_per_month <- exchange_rates_USD_per_month  

if(lead_currency != "USD"){

  for(other_currency in relevant_currencies){
  
  exchange_rates_in_lead_currency_per_year[other_currency] <- convert_exchange_rate(exchange_rates_USD_yearly_average, lead_currency, other_currency)
  exchange_rates_in_lead_currency_per_month[other_currency] <- convert_exchange_rate(exchange_rates_USD_per_month, lead_currency, other_currency)
    
  }
}

exchange_rates_per_year <- exchange_rates_in_lead_currency_per_year %>% 
  gather(one_of(relevant_currencies), key = "currency", value = "rate") %>% 
  select(year, everything()) %>% 
  add_column(lead_currency = lead_currency)

exchange_rates_per_month <- exchange_rates_in_lead_currency_per_month %>% 
  gather(one_of(relevant_currencies), key = "currency", value = "rate") %>%
  mutate(year = year(date), month = month(date)) %>% 
  select(year, month, date, everything()) %>% 
  add_column(lead_currency = lead_currency)

write_debug_information(exchange_rates_per_year, "exchange_rates_per_year")


```


```{r exchange rate plot, echo=FALSE, fig.asp=0.6, fig.cap="Development of exchange rates of the main markets for large-scale PV 2006-2016, indexed USD-XXX January 2006=1 ", fig.width=fig.width.baseline * 0.8}

exchange_rates_norm <- exchange_rates_per_month %>% 
  filter(year == params$exchange_rate_norm_year, month == 1) %>% 
  rename(norm_rate = rate) %>% 
  select(currency, norm_rate)

exchange_rates_index <- exchange_rates_per_month %>% 
  left_join(exchange_rates_norm, by = c("currency")) %>% 
  mutate(normed_rate = rate / norm_rate) %>% 
  filter(between(year, T0, T_max))

exchange_rates_index$currency <- factor(exchange_rates_index$currency, plot_order_currency)

exchange_rates_plot <- ggplot(exchange_rates_index, aes(x = date, y = normed_rate, col = currency, linetype = currency)) +
  geom_line() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_linetype_manual(values=plot_order_linetype) +
  scale_color_npg() +
  labs(x = "", 
       y = paste("Index of", lead_currency ,"exchange rate (Jan 2006)")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust = 0.5 ))

```



## Deflators


```{r calculate delfators}

## Deflators
# load yearly deflators
cpi_deflators <- read_csv(filenames$deflators) %>% 
  select(country = LOCATION, year = TIME, value = Value)

# prepare to change the values to the reference year (2017)
cpi_deflators_index_year <- cpi_deflators %>% 
  filter(year == 2017, country %in% relevant_deflators) %>% 
  select(country, value_index = value)

defl <- cpi_deflators %>% 
  left_join(cpi_deflators_index_year) %>% 
  mutate(
    currency = country_short_to_currency[country,"currency"],
    defl = value / value_index) %>% 
  select(year, currency, defl) %>% 
  filter(defl, !is.na(currency))

#rm(cpi_deflators_index_year, cpi_deflators)
```


```{r Deflator plot, echo=FALSE, fig.asp=0.6, fig.cap="Deflation", fig.width=fig.width.baseline * 0.8}
defl_plot <- subset(defl, year>=T0, year<=T_max)
defl_plot$year <- ymd(paste(defl_plot$year,"-01-01"))

ggplot(defl_plot, aes(year, defl, col = currency, linetype = currency)) + 
  geom_line() + 
  scale_linetype_manual(values=plot_order_linetype) +
  scale_color_npg() +
  labs(
    x = "Year",
    y = "Deflation relative to 2017",
    subtitle = "Deflators are based on CPIs")

```


## Cumulative sums

X is the global cumulative deployment in year t -> cummulative sum of x_global (sum of xj) per year

```{r Calculate cumulative sums, fig.cap="Cumulative sums of different datasubsets", fig.asp=0.9, fig.width=fig.width.baseline * 0.65}

x_global <- list(
  "BNEF" = projects_with_costs_in_interval, 
  "IRENA" = irena_data) %>% 
  map(calculate_cummulative_sums)

X_all_subsets <- x_global %>% 
  reduce(bind_rows) %>% 
  group_by(subset_name) %>% 
  mutate(X = cumsum(x_global))

X <- X_all_subsets %>% 
  filter(subset_name == params$capacity_data) %>% 
  select(year, X)

```

# Learning rate calculations

We calculate 3 types of average costs and learning rates in this file:

- uncorrected learning rates
- uncorrected-weighted learning rates
- corrected learing

For a detailed explanation of the theories behind the learning rates calculations and currency corrections, compare our paper.

## Method 1: Calculation of the uncorrected learning rates 

To calculate the uncorrected learning rates, we convert **all available project costs** to different currencies. We then derive the learning rate for each currency from the global average costs $C_{global,t,2017}^{i}$ of all available projects.

In Table 1 & 2 and Figure 1, we present the results of the method 1 approach.

```{r}
results_new <- list()
results <- list()
```


### Step 1: Calculation of costs in every relevant currency
In the first step, we calculate the value for each project in each relevant currency (`r currencies_print`).

```{r Calculate uncorrected learning rates}

projects_with_costs_in_currencies <- projects_with_costs_in_interval

# add columns for each currency
projects_with_costs_in_currencies[relevant_currencies] <- NA

for(j in 1:nrow(projects_with_costs_in_currencies)){
  
  project_year_j = projects_with_costs_in_currencies[[j,"year"]]
  local_currency_j = as.character(projects_with_costs_in_currencies[[j,"local_currency"]])

  projects_with_costs_in_currencies[[j, lead_currency]] <- #
    projects_with_costs_in_currencies[[j,"local_value"]] * get_exchange_rate_for_project(exchange_rates_per_year, project_year_j, local_currency_j)
  
  
  for(relevant_currency in relevant_currencies){
    if(relevant_currency!=lead_currency){
      projects_with_costs_in_currencies[[j, relevant_currency]] <- #
        projects_with_costs_in_currencies[[j, lead_currency]] / get_exchange_rate_for_project(exchange_rates_per_year, project_year_j, relevant_currency)
    }
  }
}

rm(j, project_year_j, local_currency_j)

projects_with_costs_in_currencies


```

### Step 2. Aggregate data to means and deflate

In the second step, we aggregate the data by yearly averages for each currency. The mean is a not weighted (in contrast to method 2) average of all projects.

Here we also deflate the average to 2017 real values depending if params$deflate is set true.


```{r aggregate to average costs, echo=TRUE}

average_global_costs_uncor <- projects_with_costs_in_currencies %>% 
  select(year, !!relevant_currencies) %>% 
  gather("currency", "value", !!relevant_currencies) %>% 
  group_by(currency, year) %>% 
  summarise(nominal_costs = mean(value)) %>% 
  ungroup()

```

### Step 3. Deflate to real values (2017)

Only if params\$deflate is true. Currently: `r params$deflate`

```{r}
average_global_costs_uncor <- convert_to_real_costs(average_global_costs_uncor, defl)
```

### Step 4. Calculation of learning rates

We calculate the learning rate in each interval and for each currency using a linear regression (see calculate_learning_rate()).

```{r Calculation of learning rates}

simple_learning_rates_matrix <- matrix(
  ncol = length(intervals), 
  nrow = length(relevant_currencies))

simple_learning_rates_rsquared_matrix <- simple_learning_rates_matrix

results_list <- list()

for(j in seq_along(relevant_currencies)){
  currency <- relevant_currencies[[j]]

  for(i in seq_along(intervals)){
    interval <- intervals[[i]]
    interval_name <- names(intervals[i])
    interval_years <- as.numeric(interval[1]):as.numeric(interval[2])
    
    regression_costs <- average_global_costs_uncor %>% 
      filter(year %in% !!interval_years, currency == !!currency) %>% 
      pull(average_costs)
    
    regression_cumulative_capacity <- X %>% 
      filter(year %in% interval_years) %>% 
      pull(X)
    
    l <- calculate_learning_rate(regression_costs, regression_cumulative_capacity)
    
    debug_prefix <- paste0(interval_name, "_", "simple","_")

    write_debug_information(tibble(
        costs = regression_costs, 
        capacity = regression_cumulative_capacity,
        currency = currency), 
      paste0(debug_prefix, "regression_costs_",currency))
    
    results_list[[paste(currency, interval_name)]] <- tibble(
      currency = currency,
      interval = interval_name,
      estimate = l$learning_rate,
      CI = str_glue("[{l$confint_lower};{l$confint_upper}]"),
      R2 =  l$rsquared
    )
    
    simple_learning_rates_matrix[j,i] <- l$learning_rate
    simple_learning_rates_rsquared_matrix[j,i] <- l$rsquared
    
  }
}


results_new[["simple_learning_rates"]] <- results_list %>% 
  reduce(bind_rows) %>% 
  arrange_by_currencies(table_order_currency)

rm(currency, results_list)
  
```

### Results

```{r Learning rate table, fig.cap="Simple learning rates in different intervals.", fig.width=fig.width.baseline * 0.7, fig.asp= 0.7}

print_learning_rates_result(results_new[["simple_learning_rates"]])


results[["simple_learning_rates"]] <- simple_learning_rates_matrix %>% 
  matrix_to_tibble(names(intervals), relevant_currencies, "currency") %>% 
  arrange_by_currencies(table_order_currency)

results[["simple_learning_rates_rsquared"]] <- simple_learning_rates_rsquared_matrix %>% 
  matrix_to_tibble(names(intervals), relevant_currencies, "currency") %>% 
  arrange_by_currencies(table_order_currency)



# print_learning_rates_result(
#   results[["simple_learning_rates"]], results[["simple_learning_rates_rsquared"]], 
#   "currency", "Simple learning rates in all currencies")

```

### Combined plot

To plot all learning rates in the same plot, it is necessary to norm the costs. To do this, we select the initial year's average global costs and divide all average global costs by that value. We then get 1/W for all average costs and can compare the learning rates.

```{r Combined and normed plot, echo=FALSE, fig.cap="Comparison of learning rates, as index of the average costs in 2006", fig.width=fig.width.baseline*output.size, fig.asp=0.4, out.width=out.width.default}

combined_plot_data <- average_global_costs_uncor %>% 
  inner_join(X) %>% 
  filter(year >= T0, year <= T_max) %>% 
  select(
    year, 
    average_costs, 
    currency, 
    cumulative_capacity = X) %>% 
  mutate(currency = factor(currency, plot_order_currency))

combined_plots <- combined_plot_data %>% 
  norm_average_costs() %>% 
  mutate(year_for_text = if_else(currency == "GBP", as.character(year), "")) %>% 
  get_normed_plots(intervals)


combined_plots_legend <- combined_plots[[1]] +
  theme(legend.position="bottom") +
  guides(
    col = guide_legend(nrow=1,byrow=TRUE),
    linetype = guide_legend(nrow=1,byrow=TRUE))


plot_grid(
  plot_grid(combined_plots[[1]] + theme(legend.position = "none"), 
                   combined_plots[[2]] + theme(legend.position = "none"), 
                   combined_plots[[3]] + theme(legend.position = "none"), labels = c("a", "b", "c"), 
                   rel_widths = c(1,1,1), align = "h", ncol = 3),
  get_legend(combined_plots_legend), rel_heights = c(5,1),
  ncol = 1)


```
\clearpage


## Method 2: Currency-corrected learning rates

We calculate our improved currency-corrected learning rate in the lead currency **`r lead_currency`** The resulting, single learning rate is based on the *weighted average global cost converted to lead currency* $P_{t,2017}^l$. 

### Step 1: Marketshare calculation

A notable difference to the uncorrected learning rate method, is that we weighting the average global costs by the marketshares $\delta$. For those, we have two datasource. (1) BNEF which is also used to calculate the learning rates and (2) IRENA. The IRENA data represents more capacity and is a better measure of the global market shares.

```{r Calculation of market shares}

delta_BNEF <- calculate_delta(projects_with_costs_in_interval, x_global$BNEF)
delta_IRENA <- calculate_delta(irena_data, x_global$IRENA)

if(params$capacity_data == "IRENA"){
  delta <- delta_IRENA
} else {
  delta <- delta_BNEF
}

```


#### Marketshare plots (`r params$capacity_data`)

The market share of PV deplyoment in each currency and year is relevant. To calculate the market share of different currencies, we derive delta values from BNEF and IRENA datasets. Figure 1 visualises the market shares as a function of the currency.

```{r Marketshare plots comparison, fig.cap = "Comparison of capacity additions per year and currency area in BNEF and IRENA dataset", fig.width=fig.width.baseline, fig.asp=0.45, out.width=out.width.default}

bind_rows(delta_BNEF,delta_IRENA) %>% 
  get_delta_plot(plot_type = "absolute") +
  facet_grid(cols = vars(subset_name))

bind_rows(delta_BNEF,delta_IRENA) %>% 
  get_delta_plot(plot_type = "relative", legend = TRUE) +
  facet_grid(cols = vars(subset_name))


```

```{r Marketshare plots current, fig.cap = "Capacity additions per year and currency area", fig.width=fig.width.baseline*output.size, fig.asp=0.8, out.width=out.width.default}

plot_marketshare_absolute <- get_delta_plot(delta, plot_type = "absolute")
plot_marketshare_legend <- get_legend(plot_marketshare_absolute)
plot_marketshare_absolute <- plot_marketshare_absolute + theme(legend.position = "none")

plot_marketshare_relative <-  get_delta_plot(delta, plot_type = "relative") + theme(legend.position = "none")

plot_marketshare_top_row <- plot_grid(
  plot_marketshare_absolute,
  plot_marketshare_relative,
  plot_marketshare_legend,
  labels = c("a", "b", ""), 
  rel_widths = c(1,1,0.4), align = "h", ncol = 3)


exchange_rates_plot_legend <- get_legend(exchange_rates_plot)
exchange_rates_plot_nolegend <- exchange_rates_plot + theme(legend.position = "none")

exchange_rates_plot_aligned <- align_plots(plot_marketshare_absolute, exchange_rates_plot_nolegend, align = 'v', axis = 'l')[[2]] 


plot_marketshare_bottom_row <- plot_grid(exchange_rates_plot_aligned, exchange_rates_plot_legend, labels = c('c', ''), rel_widths = c(2,0.4), align = "h", ncol = 2)

plot_grid(plot_marketshare_top_row, plot_marketshare_bottom_row, labels = c('', 'c'), ncol = 1, rel_heights = c(1, 1.2))


```


### Step 2: Calculation of the formula variable

C are observed average cost in currency i. We use the variable average_local_costs

```{r Calculation of average_local_costs or C}

## average_local_costs = C: observed average cost per MW in currency i in year t (i.e. not a subset of all projects) 

average_local_costs <- projects_with_costs_in_interval %>% 
  group_by(year, local_currency) %>% 
  summarise(C = mean(local_value)) %>% ungroup() %>%
  rename(currency = local_currency) %>% 
  complete(year, currency, fill = list(C = 0))

```

The table shows which currency have average_local_costs in which year. 

### Step 3: Aggregation to weighted average global costs (P)

In the third step, we aggregate the data by yearly averages for each currency. The mean is weighted by the marketshare $\delta$ (in contrast to method 1). We calculate two different $P$: 

- $P$ for the uncorrected-weighted average global costs
- $P_{tilde}$ for the corrected average global costs

```{r Calculation of the weighted average global cost P}

## w: exchange rate between currency i and the lead currency l in price notation
# e.g. "5 lead-dollars / 1 Euro" 

# I changed to variable names that are used in the paper
C <- average_local_costs

numeric_P_comp <- numeric(length(all_years) * length(relevant_currencies))

alpha <- tibble(
  "currency" = rep(relevant_currencies, length(all_years)),
  "year" = rep(all_years, length(relevant_currencies)),
  "alpha" = numeric_P_comp
)

P_comp <- tibble(
  "lead_currency" = lead_currency,
  "currency" = rep(relevant_currencies, length(all_years)),
  "year" = rep(all_years, length(relevant_currencies)),
  "nominal_costs" = numeric_P_comp
) %>% inner_join(defl, by = c("currency", "year"))

P_comp_tilde <- P_comp

for(t in all_years){
  
  for(i in relevant_currencies){

    delta_i_t <- filter(delta, year == t, currency == i) %>% pull(delta)
    w_i_t <- filter(exchange_rates_per_year, year == t, currency == i) %>% pull(rate)
    w_i_0 <- filter(exchange_rates_per_year, year == T0, currency == i) %>% pull(rate)
    C_i_t <- filter(C, year == t, currency == i) %>% pull(C)
    
    P_comp[P_comp$year == t & P_comp$currency == i, "nominal_costs"] <- delta_i_t * w_i_t * C_i_t
    P_comp_tilde[P_comp$year == t & P_comp$currency == i, "nominal_costs"] <- delta_i_t * w_i_0 * C_i_t

  }
  
}

write_debug_information(P_comp, paste0("P_comp_nominal"))

P <- calculate_P(P_comp)
P_tilde <- calculate_P(P_comp_tilde)


```

### Step 4. Deflate to real values

We defalte after the currency conversion. I.e. the learning rates of multiple base currencies will only be the same for nominal costs. For real costs, the deflator of different base currencies lead to different learning rates.

Only if params\$deflate is true. Currently: `r params$deflate`

```{r}
P <- convert_to_real_costs(P, defl)
P_tilde <- convert_to_real_costs(P_tilde, defl)
```


### Step 5: Calculation of the learning rate

A regression over $P_{t,2017}^l$ gives us the **un**corrected global learning rates in `r lead_currency`,. The **un**corrected rate includes both technological learning and changes in exchange rates. Therefore, it is different in different lead currencies. To get the same number in every lead currency (i.e. a truly global learning rate), we adjust the uncorrected-weighted global learning rate for the currency effect. Applying $\alpha$ to the learning rate calculation results in the corrected global learning rate. The corrected rate only includes technological learning.

```{r Calculation of uncorrected-weighted and corrected learning rate}

corrected_learning_rates_list <- list()
corrected_learning_rates_rsquared_list <- list()

types <- c("uncorrected-weighted","corrected")

results_list <- list()


for(type in types){
  corrected_learning_rates_list[[type]] <- list()
  corrected_learning_rates_rsquared_list[[type]] <- list()
  
  for(i in 1:length(intervals)){
    interval <- intervals[[i]]
   
    interval_years <- as.numeric(interval[1]):as.numeric(interval[2])
    interval_name <- names(intervals[i])
    
    if(type == "uncorrected-weighted"){
      relevant_P <- P
    } else {
      relevant_P <- P_tilde
    }
    
    regression_costs <- relevant_P %>% 
      filter(year %in% interval_years) %>% 
      pull(average_costs)
    
    regression_cumulative_capacity <- X %>% 
      filter(year %in% interval_years) %>% 
      pull(X)
    
    l <- calculate_learning_rate(regression_costs, regression_cumulative_capacity)
    
    results_list[[paste(type, interval_name)]] <- tibble(
      type = type,
      interval = interval_name,
      estimate = l$learning_rate,
      CI = str_glue("[{l$confint_lower};{l$confint_upper}]"),
      R2 =  l$rsquared
    )
    
    debug_prefix <- paste0(interval_name, "_", type,"_")
    
    write_debug_information(tibble(
      costs = regression_costs, 
      capacity = regression_cumulative_capacity), paste0(debug_prefix, "regression_costs"))

    corrected_learning_rates_list[[type]][[interval_name]] <- l$learning_rate
    corrected_learning_rates_rsquared_list[[type]][[interval_name]] <- l$rsquared
    
  }
}


results_new[["corrected_learning_rates"]] <- results_list %>% 
  reduce(bind_rows)

rm(results_list)
```

## Method 2: Results

We get the following learning rates using the corrected learning rates approach:

```{r Results table}

print_learning_rates_result(results_new[["corrected_learning_rates"]])


results[["corrected_learning_rates"]] <- corrected_learning_rates_list %>% 
  nested_list_to_matrix() %>% 
  matrix_to_tibble(names(intervals), types, "type")

results[["corrected_learning_rates_rsquared"]] <- corrected_learning_rates_rsquared_list %>% 
  nested_list_to_matrix() %>% 
  matrix_to_tibble(names(intervals), types, "type")

#print_learning_rates_result(
#  results[["corrected_learning_rates"]], results[["corrected_learning_rates_rsquared"]], 
#  "type", paste("Global learning rates in lead currency", lead_currency))

```

## Combined plot of simple and corrected learning rates

Comparison with the simple learning rates for every country

```{r Combined plot, fig.cap="Learning rates plot", fig.asp=0.4}

mypal = pal_npg("nrc")(length(relevant_currencies))

corrected_rates <- results_new$corrected_learning_rates %>% 
  filter(type == "corrected") %>%
  select(corrected_lr = estimate, interval)

combined_plot_data <- bind_rows(
    results$corrected_learning_rates %>% 
      filter(type == "corrected") %>% 
      add_column(currency = "fx-corrected"),
    results$simple_learning_rates %>% 
      add_column(type = "uncorrected")
    ) %>% 
  add_row(currency = "", type = "placeholder", .after = 1) %>% 
  gather(key = "interval", value = "lr", contains("-")) %>% 
  left_join(corrected_rates, by = c("interval")) %>% 
  mutate(
    lr = round(replace_na(lr, replace = 0),digits = 1),
    diff = round(lr - corrected_lr, digits = 1),
    diff_text = paste0(if_else(diff > 0, "+", ""), diff),
    interval = fct_relevel(interval, names(intervals)),
    currency = fct_relevel(currency, "fx-corrected", after = 0)) 

plots <- list()

for(i in seq_along(intervals)){
  
  interval <- intervals[[i]]
  interval_name <- names(intervals[i])
  interval_years <- as.numeric(interval[1]):as.numeric(interval[2])

  corrected_rate <- corrected_rates %>% 
    filter(interval == interval_name) %>% 
    pull(corrected_lr)
  
  plots[[interval_name]] <- combined_plot_data %>% 
    filter(interval == interval_name) %>% 
    ggplot(aes(fill = currency, y = lr, x = currency)) + 
      geom_bar(position="dodge", stat="identity") +
      geom_hline(yintercept = corrected_rate) + 
      geom_segment(. %>% filter(currency %in% relevant_currencies), mapping=aes(x = currency, xend=currency, y = corrected_lr, yend=lr), 
                   arrow = arrow(length = unit(0.05, "inches"), type = "closed"), 
                   size = 1, color="black") +
      geom_text(. %>% filter(currency != ""), mapping = aes(y = lr/2, label = lr), col = "white", size = 3) + # learning rates
      geom_label_repel(. %>% filter(currency %in% relevant_currencies), mapping = aes(label = diff_text), nudge_y = 2, fill = "white", size = 3) + # differences
      scale_fill_manual(values = c("#000000", "white", mypal)) +
      scale_y_continuous(limits = c(0,40), expand = expand_scale(mult = c(0, 0.1))) +
      labs(
        subtitle = interval_name, y = paste0("Learning rate in %")) +
      theme(
        axis.title.x = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(), 
        panel.grid.major = element_blank())
}

# TODO: integrate in get_legend function
plot_legend <- plots[[1]] +
  theme(legend.position="bottom") +
  guides(
    fill = guide_legend(nrow=1,byrow=TRUE))

plot_grid(
  plot_grid(plots[[1]] + theme(legend.position = "none"), 
                   plots[[2]] + theme(legend.position = "none"), 
                   plots[[3]] + theme(legend.position = "none"), labels = c("a", "b", "c"), 
                   rel_widths = c(1,1,1), align = "h", ncol = 3),
  get_legend(plot_legend), rel_heights = c(5,1),
  ncol = 1)


```

```{r Save results to csv}

if(!params$run_test){

  
  # simple learning rates
  r1 <- results$simple_learning_rates %>% 
    gather(starts_with('2'), key = interval, value = rate) 
  
  r2 <- results$simple_learning_rates_rsquared %>% 
    gather(starts_with('2'), key = interval, value = rsquared)
  
  lr_result_1 <- left_join(r1, r2, by = c("currency", "interval")) %>% 
      add_column(type = "uncorrected")
  
  # corrected learning rates
  r1 <- results$corrected_learning_rates %>%
    gather(starts_with('2'), key = interval, value = rate) 
  
  r2 <- results$corrected_learning_rates_rsquared %>%
    gather(starts_with('2'), key = interval, value = rsquared)
  
  lr_result_2 <- left_join(r1, r2, by = c("interval", "type")) %>% 
    add_column(currency = params$lead_currency) 
  
  
  bind_rows(lr_result_1,lr_result_2) %>% 
    add_column(version = version) %>% 
    write_csv(paste0("output/results/learning_rate_results_",params$lead_currency,"_",version,".csv"))

}

```

