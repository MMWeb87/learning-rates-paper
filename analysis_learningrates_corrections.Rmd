---
title: "The dependency of learning rates on exchange rate fluctuations and a method to correct them"
editor_options:
  chunk_output_type: inline
output:
  html_document:
    df_print: paged
    toc_depth: '2'
  html_notebook: default
  pdf_document:
    highlight: zenburn
    keep_tex: yes
    number_sections: yes
    toc_depth: 2
params:
  capacity_threshold: 5
  delta: IRENA
  x_global_for_cumsum: IRENA
  X_norm: no
  X_norm_capacity: 500
  X_norm_year: 2006
  lead_currency: USD
  always_use_yearly_er: yes
  exchange_rate_norm_year: 2010
  default_digits: 2
  debug: no
  use_kable: no
subtitle: Sourcefile and calculations
bibliography: bib_lc.bib
---

```{r setup, include=FALSE}

library(knitr)
library(ggsci)
library(gridExtra)
library(tidyverse)
library(lubridate)
library(cowplot)

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
theme_set(theme_bw(base_size = 9.5) + theme(legend.title=element_blank()))

```


```{r parameters}

# to accept these relevant currencies, they need to be in the currency_translation table
relevant_currencies <- c("CNY","EUR","GBP","INR","JPY","USD")

table_order_currency <- c("USD", "EUR", "JPY", "CNY", "INR", "GBP")

plot_order_currency <- relevant_currencies
plot_order_country <- c("China", "Euro Area", "United Kingdom", "India", "Japan", "United States", "ROW")
plot_order_linetype <- c("dotdash", "longdash", "dashed", "dotted", "twodash", "solid")


intervals <- list(
  c(2006,2011),
  c(2012,2016),
  c(2006,2016)
)

fig.width.baseline <- 7
out.width.default <- "100%"

combined_plot_interval <- "2006-2016"

filenames <- list(
  exchange_rates = "input/USD_exchange_rates.csv",
  irena_data = "input/irena-capacity-generation.csv",
  currency_to_currency_area_translation = "input/currency_to_currency_area_translation.csv"
)


```

```{r Init variables, echo=FALSE, include=FALSE}

source("functions_learning.R")

# T0 is the start year and T_max the end year of the interval
# lead currency is the currency of the learning rate
T0 <- min(unlist(intervals))
T_max <- max(unlist(intervals))
all_years <- as.numeric(T0):as.numeric(T_max)

lead_currency <- params$lead_currency

currencies_print <- paste(relevant_currencies, collapse=", ")
all_subset_intervals <- list(
  i0 = c(paste0(T0,"-01-01"), paste0(T_max,"-01-02"))
)

names(intervals) <- make_interval_names(intervals)

```

# Results for PV -- in lead currency `r params$lead_currency` & with `r params$delta` marketshare 

This file documents the method of the paper:  . In it,  we present an improved approach that uses global market shares (in this document based on data from `r params$delta`) and a correction factor to calculate a global learning rate that is the same over all currencies. 

## Parameters

We chose `r length(relevant_currencies)` relevant currencies (`r currencies_print`) and calculate a learning rate between `r T0` and `r T_max`. We look only at PV projects above `r params$capacity_threshold` MW.

# Data

## Load and prepare BNEF data

Our analysis is based on the Bloomberg New Energy Finance (BNEF) renewable energy database. The dataset is copyright protected and can not be shared. We use the BNEF dataset because it gives investment costs for individual installations and projects in the original currencies, or allows for conversion to the domestic currency of each project.

As base unit, we use local value per capacity (e.g. USD/W)

```{r Read BNEF dataset}

data_projects <- read_csv(
    file = "input/data_PV_all_nominal.csv", 
    col_types = cols(
      `Financing Date` = col_date(format="%Y-%m-%d")),
    na = c("","NA")) %>%
  select(
    id = `Renewable Project ID`,
    capacity = `Capacity - total (MWe)`,
    total_local_value = `Total Value (Local)`,
    total_usd_value = `Total Value ($m)`,
    local_currency_long = `Local Currency`,
    country = Country,
    date = `Financing Date`,
    status = Status) %>% 
  mutate(
    local_value = total_local_value / capacity, # from million currency and MW to currency/W
    year = year(date)
  ) %>% 
  add_column(subset_name = "All projects")


```

We subset the data according to different criteria:
 
```{r subset BNEF dataset, echo=TRUE}

commissioned_projects <- filter(data_projects, 
    status == "Commissioned") %>% 
  mutate(subset_name = "BNEF commissioned")

large_projects <- filter(commissioned_projects,
    capacity >= params$capacity_threshold) %>% 
  mutate(subset_name = "BNEF >= 5MW")

relevant_projects <- filter(large_projects,
    !is.na(date)) %>% 
  mutate(subset_name = "BNEF with date")

non_relevant_projects1 <- filter(commissioned_projects,
    capacity < params$capacity_threshold) %>% 
  mutate(subset_name = "BNEF non-relevant")

non_relevant_projects2 <- filter(commissioned_projects,
    capacity >= params$capacity_threshold,
    is.na(date)) %>% 
  mutate(subset_name = "BNEF non-relevant")

```

### Completion of dataset with currencies

We consistently use the short ISO currency codes. Hence we convert the long currency codes to short ones.


The BNEF dataset does not consistenly report the original, local currencies. In cases where the local currency is missing, we assumed that projects were financed in the main currency of the corresponding project countries. Our assumption is justified by an inital data exploration that shows that this correlation holds true for most projects that had the currency information in the first place.

```{r missing currency record}

filter(relevant_projects, !is.na(total_local_value), is.na(local_currency_long))

```

For the relevant_projects, this seems not to be an issue. Let's do this nevertheless, in case we would want to use a larger dataset to calculate the marketshare with BNEF.

```{r Fill missing currency records in BNEF}

# I map short to long currency codes because it's easier to read. 
long_to_short_currencycode_mappingtable <- read_csv("input/currency_translation.csv") %>%
  select(local_currency_short = short, local_currency_long = long)
country_to_currencycode_mappingtable <- read_csv("input/country_to_currency_translation.csv")

relevant_projects_completed <- relevant_projects %>% 
  left_join(
    long_to_short_currencycode_mappingtable, 
    by = "local_currency_long") %>% 
  left_join(
    select(country_to_currencycode_mappingtable, country = country_long, local_currency_added = currency), 
    by = "country") %>% 
  mutate(
    local_currency_final = ifelse(is.na(local_currency_short), local_currency_added, local_currency_short))

# In these projects, I decided to keep to currency that was originally reported to be more accurate.
# relevant_projects_completed %>% filter(local_currency_short != local_currency_added)

# To simplify thing, I change the naming here for the final subset
projects_with_costs_with_outliers <- relevant_projects_completed %>% 
  select(id, date, capacity, local_currency = local_currency_final, local_value, year) %>% 
  filter(
    !is.na(local_value),
    !is.na(local_currency),
  local_currency != "Other"
  ) %>% 
  mutate(subset_name = "BNEF with costs with outliers")

projects_with_costs_with_outliers$local_currency <- as.factor(projects_with_costs_with_outliers$local_currency)


country_short_to_currency <- data.frame(currency = country_to_currencycode_mappingtable$currency)
rownames(country_short_to_currency) <- country_to_currencycode_mappingtable$country_short

```

### Outlier treatment

```{r Outlier treatment, fig.cap="Overview of BNEF data and outliers.", fig.width=fig.width.baseline}

outliers_table <- read_csv("input/outliers.csv", col_names = c("id", "desc"), skip = 1)

projects_with_costs_with_outliers <- projects_with_costs_with_outliers %>%  
  mutate(outlier = id %in% outliers_table$id)

n_outliers <- sum(projects_with_costs_with_outliers$outlier)

# projects_with_costs
projects_with_costs <- projects_with_costs_with_outliers %>% 
  filter(outlier == FALSE) %>% 
  select(-outlier) %>% 
  mutate(subset_name = "BNEF with costs")

overview_plots_after <- list()
overview_plots_after_boxplot <- list()

for (var in unique(projects_with_costs_with_outliers$local_currency)) {
  
  overview_plots_after[[var]] <- projects_with_costs_with_outliers %>% 
    filter(local_currency == var) %>% 
    ggplot(aes(date, local_value, shape = outlier, col = outlier)) +
      geom_point() +
      scale_shape_manual(values = c(16, 9)) +
      scale_color_manual(values = c("black", "red")) +
      labs(x = "Year", 
           y = paste("Costs in", var),
          subtitle = paste("Projects reported in", var)) +
      theme(legend.position = "none")
}

with(overview_plots_after, grid.arrange(CNY,EUR,INR,JPY,GBP,USD, ncol=3, widths=c(1, 1, 1)))
```

From *projects_with_costs_with_outliers*, we exclude `r n_outliers` projects as outliers because they have implausible costs that deviate very strongly (a factor 10 or more) from the overall trend and have most likely been entered incorrectly in the BNEF database. Compare the outliers.csv for details and see the plots bellow.

### Apply interval

```{r}

projects_with_costs_in_interval <- filter(projects_with_costs,
  between(year, T0, T_max)) %>% 
  mutate(subset_name = "BNEF with costs")

```


### Overview

```{r Statistical number of BNEF}

stats <- list()

# Different BNEF statistics
stats[["number"]][["all_projects"]] <- nrow(data_projects)
stats[["capacity"]][["all_projects"]] <- sum(data_projects$capacity, na.rm = T)

stats[["number"]][["commissioned_projects"]] <- nrow(commissioned_projects)
stats[["capacity"]][["commissioned_projects"]] <- sum(commissioned_projects$capacity, na.rm = T)

stats[["number"]][["large_projects"]] <- nrow(large_projects)
stats[["capacity"]][["large_projects"]] <- sum(large_projects$capacity)

stats[["number"]][["relevant_projects"]] <- nrow(relevant_projects)
stats[["capacity"]][["relevant_projects"]] <- sum(relevant_projects$capacity)

stats[["number"]][["non_relevant_projects1"]] <- nrow(non_relevant_projects1)
stats[["capacity"]][["non_relevant_projects1"]] <- sum(non_relevant_projects1$capacity)

stats[["number"]][["non_relevant_projects2"]] <- nrow(non_relevant_projects2)
stats[["capacity"]][["non_relevant_projects2"]] <- sum(non_relevant_projects2$capacity)

stats[["number"]][["relevant_projects_with_costs_with_outliers"]] <- nrow(projects_with_costs_with_outliers)
stats[["capacity"]][["relevant_projects_with_costs_with_outliers"]] <- sum(projects_with_costs_with_outliers$capacity)

stats[["number"]][["relevant_projects_with_costs"]] <- nrow(projects_with_costs)
stats[["capacity"]][["relevant_projects_with_costs"]] <- sum(projects_with_costs$capacity)

stats[["number"]][["relevant_projects_with_costs_in_interval"]] <- nrow(projects_with_costs_in_interval)
stats[["capacity"]][["relevant_projects_with_costs_in_interval"]] <- sum(projects_with_costs_in_interval$capacity)


# Shares
stats[["number_share"]] <- stats$number/stats[["number"]][["commissioned_projects"]]
stats[["capacity_share"]] <- stats$capacity/stats[["capacity"]][["commissioned_projects"]]


## Overview table
summary_table_rows <- 5
summary_table <- tibble(
  "Filter criterium" = character(summary_table_rows),
  "Projects" = numeric(summary_table_rows),
  "Share projects" = numeric(summary_table_rows),
  "Capacity" = numeric(summary_table_rows),
  "Share capacity" = numeric(summary_table_rows))

summary_table[1,] <- list("All BNEF projects", stats[["number"]][["all_projects"]], NA, stats[["capacity"]][["all_projects"]], NA)


summary_table[2,]  <- list("Commissioned projects", stats[["number"]][["commissioned_projects"]], 1, stats[["capacity"]][["commissioned_projects"]], 1)

summary_table[3,]  <- list("Capacity >= 5MW", 
                           stats[["number"]][["large_projects"]],
                           stats[["number_share"]][["large_projects"]],
                           stats[["capacity"]][["large_projects"]],
                           stats[["capacity_share"]][["large_projects"]])


summary_table[4,]  <- list("With financing date reported", 
                           stats[["number"]][["relevant_projects"]],
                           stats[["number_share"]][["relevant_projects"]],
                           stats[["capacity"]][["relevant_projects"]],
                           stats[["capacity_share"]][["relevant_projects"]])

summary_table[5,]  <- list("With financing costs reported", 
                           stats[["number"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["number_share"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["capacity"]][["relevant_projects_with_costs_with_outliers"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs_with_outliers"]])

summary_table[6,]  <- list("Without outliers", 
                           stats[["number"]][["relevant_projects_with_costs"]],
                           stats[["number_share"]][["relevant_projects_with_costs"]],
                           stats[["capacity"]][["relevant_projects_with_costs"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs"]])

summary_table[7,]  <- list(paste("Between", names(intervals[3])), 
                           stats[["number"]][["relevant_projects_with_costs_in_interval"]],
                           stats[["number_share"]][["relevant_projects_with_costs_in_interval"]],
                           stats[["capacity"]][["relevant_projects_with_costs_in_interval"]],
                           stats[["capacity_share"]][["relevant_projects_with_costs_in_interval"]])

# Remove all projects for report. In GW.
summary_table_print <- summary_table %>% 
  mutate(Capacity  = Capacity/1000) %>% 
  slice(-1)
  

print_number_table(summary_table_print, caption = "Summary of our subsets of the BNEF dataset. All lower rows include the filter criteria from the upper rows. Capacity in GW.", digits = 2)


```

The BNEF dataset contains a total of `r stats[["number"]][["all_projects"]]` PV projects, of which `r stats[["number"]][["commissioned_projects"]]` are commissioned. Of those commissioned projects, we consider `r stats[["number"]][["relevant_projects"]]` as relevant because they have a capacity of >= `r params$capacity_threshold` MW. Among the comissioned projects, some are not relevant (to caluclate learning rates) because they are either below that threshold (`r stats[["number"]][["non_relevant_projects1"]]`), above but miss a financing date record (`r stats[["number"]][["non_relevant_projects2"]]`) or are reported in other, minor currencies. 

Finally, only **`r stats[["number"]][["relevant_projects_with_costs_in_interval"]]` projects** have a financing costs in one of the relevant currencies and our interval. We use these projects , i.e. `r round(stats[["capacity_share"]][["relevant_projects_with_costs_in_interval"]]*100,1)`% of worldwide capacity to calculate the learning rates. These numbers and the corresponding capacity they represent are sumarised in the table.


## Load and prepare IRENA data

If the parameter is set, we also use the IREAN dataset to calculate market shares. The IRENA data better represent the market shares than the BNEF data. For the calculations of the learning rates we need curreny information that the IRENA dataset does not include. Hence, we assume that projects in a country were financed in the main currencies. Compare Step 1 of Method 2 for the graph.

```{r Load and prepare IRENA data}

# The irena dataset need to have the same format and apply the same criteria as the BNEF dataset (especially capacity threshold)

irena_data <- read_csv(filenames$irena_data) %>% 
  filter(
    Product == "Solar photovoltaic" | Product == "Solar Photovoltaic", 
    Type == "ONG",
    `Capacity (MW)` >= params$capacity_threshold) %>% 
  select(
    capacity_cum = `Capacity (MW)`,
    country = ISO, 
    year = Year) %>%
  arrange(country, year) %>% # need to sort, as I calculate the difference between each year in a loop
  mutate(capacity = capacity_cum) %>% # the simplest way to set the start capacity in each year 
  add_column(local_currency = NA) %>% 
  add_column(subset_name = "IRENA")

irena_data$local_currency <- as.character(irena_data$local_currency)


# We loop through the dataset, because we can both set the irena_data$local_currency and calculate yearly capacity additions instead of the cummulative capacity we have so far.
for(i in 1:nrow(irena_data)){
  
  ith_irena_country <- as.character(irena_data[i,"country"])

  # If the country is in the "country_to_currency_translation" list, it is relevant (rest ROW)
  if(ith_irena_country %in% rownames(country_short_to_currency)){
    irena_data[i,"local_currency"] <- as.character(country_short_to_currency[ith_irena_country,"currency"])
  } else {
    irena_data[i,"local_currency"] <- "Other"
  }

  if (i > 1){

    if(irena_data[i-1,"country"] == irena_data[i,"country"]){
      irena_data[i,"capacity"] <- irena_data[i,"capacity_cum"] - irena_data[i-1,"capacity_cum"]
    }    
  }  
}

rm(i,ith_irena_country)

```



## Exchange rates

Exchange rates are from OFX and OECD, and the defaltors are based on OECD Consumer Price Index (CPI) data. We take the yearly average as basis for our calculations

```{r load exchange rates}

exchange_rates_USD <- read_csv(filenames[["exchange_rates"]], col_types = cols(
    date = col_date(format = "%d.%m.%y"))) %>% 
  drop_na() %>% 
  add_column(USD = 1) %>% 
  select(date, !!relevant_currencies)

exchange_rates_in_lead_currency <- exchange_rates_USD  

if(lead_currency != "USD"){

  for(other_currencies in relevant_currencies){
    # c_curr/c_lead = c_curr/USD (column c_curr in relevant_exchange_rates) * (c_lead/USD)^-1 (column c_lead in relevant_exchange_rates)
  exchange_rates_in_lead_currency[other_currencies] <- exchange_rates_USD[other_currencies] * as_tibble(exchange_rates_USD[lead_currency]^(-1))

  }
}

rm(other_currencies)

# Select the lead currencies exchange rate
exchange_rates_in_lead_currency <- exchange_rates_in_lead_currency %>% 
  gather(one_of(relevant_currencies), key = "currency", value = "rate") %>% 
  mutate(month = month(date), year = year(date)) %>% 
  select(date, year, month, everything())

exchange_rates_averages <- exchange_rates_in_lead_currency %>% 
  group_by(year, currency) %>% 
  summarise(rate = mean(rate)) %>% ungroup() %>% 
  add_column(lead_currency = lead_currency)

# In case I want to change how the yearly exchange rate is calculated
exchange_rates_per_year <- exchange_rates_averages
exchange_rates_per_month <- exchange_rates_in_lead_currency

```


```{r exchange rate plot, echo=FALSE, fig.asp=0.6, fig.cap="Comparison of all different exchange rates", fig.width=fig.width.baseline * 0.8}

exchange_rates_norm <- exchange_rates_per_year %>% 
  filter(year == params$exchange_rate_norm_year) %>% 
  rename(norm_rate = rate) %>% 
  select(currency, norm_rate)

exchange_rates_index <- exchange_rates_per_month %>% 
  left_join(exchange_rates_norm, by = c("currency")) %>% 
  mutate(normed_rate = rate / norm_rate) %>% 
  filter(between(year, T0, T_max))

exchange_rates_index$currency <- factor(exchange_rates_index$currency, plot_order_currency)

ggplot(exchange_rates_index, aes(x = date, y = normed_rate, col = currency, linetype = currency)) +
  geom_line() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_linetype_manual(values=plot_order_linetype) +
  scale_color_npg() +
  labs(x = "", 
       y = paste("Index of", lead_currency ," exchange rate"),
      subtitle = paste("The data is an index of the avaerage exchange rate of", params$exchange_rate_norm_year)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust = 0.5 ))

```

## Deflators


```{r calculate delfators}

## Deflators
# load yearly deflators
cpi_deflators <- read.csv("input/OECD_CPI_yearly.csv", stringsAsFactors = F)
cpi_deflators <- select(cpi_deflators, country = LOCATION, year = TIME, value = Value)
cpi_deflators$year <- as.numeric(cpi_deflators$year)

# change the values to the reference year (2017)
cpi_deflators_index_year <- select(filter(cpi_deflators, year == 2017), country, value_index = value)
cpi_deflators <- merge(cpi_deflators, cpi_deflators_index_year)

# add new variables (currency and indexed value)
cpi_deflators <- mutate(
  cpi_deflators, 
  currency = sapply(country, convert_location_to_currency),
  defl = value / value_index)

# prepare final deflator object
defl <- select(cpi_deflators, year, currency, defl)
defl <- filter(defl, !is.na(currency))
defl$currency <- as.factor(defl$currency)

rm(cpi_deflators_index_year, cpi_deflators)
```


```{r Deflator plot, echo=FALSE, fig.asp=0.6, fig.cap="Deflation", fig.width=fig.width.baseline * 0.8}
defl_plot <- subset(defl, year>=T0, year<=T_max)
defl_plot$year <- ymd(paste(defl_plot$year,"-01-01"))

ggplot(defl_plot, aes(year, defl, col = currency, linetype = currency)) + 
  geom_line() + 
  scale_linetype_manual(values=plot_order_linetype) +
  scale_color_npg() +
  labs(
    x = "Year",
    y = "Deflation relative to 2017",
    subtitle = "Deflators are based on CPIs")

```


## Cumulative sums

X is the global cumulative deployment in year t -> cummulative sum of x_global (sum of xj) per year

```{r Calculate cumulative sums, fig.cap="Cumulative sums of different datasubsets", fig.asp=0.9, fig.width=fig.width.baseline * 0.65}

x_global <- list(
#    "BNEF_commissioned" = commissioned_projects,
    "BNEF_relevant" = relevant_projects,
    "BNEF_interval" = projects_with_costs_in_interval,
    "IRENA" = irena_data) %>% 
  map(calculate_cummulative_sums)

X_all_subsets <- x_global %>% 
  reduce(bind_rows) %>% 
  group_by(subset_name) %>% 
  mutate(X = cumsum(x_global))

# to decide which cumsums to take
X_all_subsets %>%  
  filter(between(year, T0, T_max)) %>% 
  ggplot(mapping = aes(x = year, y = X/1000)) +
    geom_line(mapping = aes(linetype = subset_name, colour = subset_name)) +
    labs(
      y = "Cumulative PV (>= 5MW) deployment (GW)",
      colour = "Dataset or subset", linetype = "Dataset or subset") +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank())

X <- X_all_subsets %>% 
  filter(subset_name == params$x_global_for_cumsum) %>% 
  select(year, X)


if(params$X_norm){
  normyear_value <- filter(X, year == params$X_norm_year) %>% pull(X)
  X <- X %>% 
    mutate(X = X - normyear_value + params$X_norm_capacity)
}
  
```

To get a good estimate of the learning rate, the underyling cummulative capacity should reflect the actual global deplyoment. If we took a subset, the learning rate would be overestimated. The plot indicates that there is a large difference in the global cummulative deplyoment in the subsets. So:

- if only working with the BNEF dataset, we take BNEF relevant, as they only consider 5MW projects. They fulfill our assumption that large projects are global. 

- to improve results, we should take the IRENA data.

# Learning rate calculations

For a detailed explanation of the theories behind the learning rates calculations and currency corrections, compare our paper.

## Method 1: Calculation of the uncorrected learning rates 

To calculate the simple learning rates, we convert **all available project costs** to different currencies. We then derive the learning rate for each currency from the global average costs $C_{global,t,2017}^{i}$ of all available projects.

In Table 1 & 2 and Figure 1, we present the results of the method 1 approach.

### Step 1: Calculation of costs in every relevant currency
In the first step, we calculate the value for each project in each relevant currency (`r currencies_print`).

1. We add new variables for every relevant currency.
2. For each project, we calculate the value of the lead currency (`r params$lead_currency`) from the local value and an exchange rate that is either close to the project date or fixed for each year (if always_use_yearly_er == yes). To be consistent with method 2, we use a fixed exchange rate per year in our paper.
3. We caluculate the values in each relevant currency from the lead currency value and the exchange rates (see 2).

```{r Calculate simple learning rates}

projects_with_costs_in_currencies <- projects_with_costs_in_interval

# add columns for each currency
projects_with_costs_in_currencies[relevant_currencies] <- NA

for(j in 1:nrow(projects_with_costs_in_currencies)){
  
  project_date_j = projects_with_costs_in_currencies[[j,"date"]]
  local_currency_j = as.character(projects_with_costs_in_currencies[[j,"local_currency"]])

  projects_with_costs_in_currencies[[j, lead_currency]] <- #
    projects_with_costs_in_currencies[[j,"local_value"]] * get_exchange_rate_for_project(project_date_j, local_currency_j)
  
  
  for(relevant_currency in relevant_currencies){
    if(relevant_currency!=lead_currency){
      projects_with_costs_in_currencies[[j, relevant_currency]] <- #
        projects_with_costs_in_currencies[[j, lead_currency]] / get_exchange_rate_for_project(project_date_j, relevant_currency)
    }
  }
}

rm(j, project_date_j, local_currency_j)

projects_with_costs_in_currencies


```

### Step 2. Aggregate data to means

In the second step, we aggregate the data by yearly averages for each currency. The mean is a not weighted (in contrast to method 2) average of all projects.

```{r aggregate to average costs, echo=TRUE}

average_global_costs <- projects_with_costs_in_currencies %>% 
  select(year, !!relevant_currencies) %>% 
  gather("currency", "value", !!relevant_currencies) %>% 
  group_by(currency, year) %>% 
  summarise(nominal_costs = mean(value)) %>% ungroup()

```

### Step 3. Convert to real values (in 2017 units)

We convert this the average in national nominal currency to the real 2017 values for each base currency.

```{r}

average_global_costs_real <- convert_to_real_costs(average_global_costs, defl)

```

### Step 4. Calculation of learning rates

We calculate the learning rate in each interval and for each currency using a linear regression (see calculate_learning_rate()).

```{r Calculation of learning rates}

simple_learning_rates_matrix <- matrix(
  ncol = length(intervals), 
  nrow = length(relevant_currencies))

simple_learning_rates_rsquared_matrix <- simple_learning_rates_matrix

for(j in seq_along(relevant_currencies)){
  currency <- relevant_currencies[[j]]

  for(i in seq_along(intervals)){
    interval <- intervals[[i]]
    interval_name <- names(intervals[i])
    interval_years <- as.numeric(interval[1]):as.numeric(interval[2])
    
    regression_costs <- average_global_costs_real %>% 
      filter(year %in% !!interval_years, currency == !!currency) %>% 
      pull(real_costs)
    
    regression_cumulative_capacity <- X %>% 
      filter(year %in% interval_years) %>% 
      pull(X)
    
    l <- calculate_learning_rate(regression_costs, regression_cumulative_capacity)
    
    simple_learning_rates_matrix[j,i] <- l$learning_rate
    simple_learning_rates_rsquared_matrix[j,i] <- l$rsquared
  }
}

rm(currency)
  
```

### Results

```{r Learning rate table, fig.cap="Simple learning rates in different intervals.", fig.width=fig.width.baseline * 0.7, fig.asp= 0.7}

results <- list()

results[["simple_learning_rates"]] <- simple_learning_rates_matrix %>% 
  matrix_to_tibble(names(intervals), relevant_currencies, "currency") %>% 
  arrange_by_currencies(table_order_currency)

results[["simple_learning_rates_rsquared"]] <- simple_learning_rates_rsquared_matrix %>% 
  matrix_to_tibble(names(intervals), relevant_currencies, "currency") %>% 
  arrange_by_currencies(table_order_currency)

  
print_learning_rates_result(
  results[["simple_learning_rates"]], results[["simple_learning_rates_rsquared"]], 
  "currency", "Simple learning rates in all currencies")

```

\clearpage


## Method 2: Currency-corrected learning rates

We calculate our improved currency-corrected learning rate in the lead currency **`r lead_currency`** The resulting, single learning rate is based on the *weighted average global cost converted to lead currency* $P_{t,2017}^l$. 

### Step 1: Marketshare calculation

A notable difference to the uncorrected learning rate method, is that we weighting the average global costs by the marketshares $\delta$. For those, we have two datasource. (1) BNEF which is also used to calculate the learning rates and (2) IRENA. The IRENA data represents more capacity and is a better measure of the global market shares.

```{r Calculation of market shares}

currency_to_currency_area_translation <- read_csv(filenames$currency_to_currency_area_translation) %>% 
  select(currency = "currency", currency_area) %>% 
  mutate(currency_area = fct_relevel(currency_area, plot_order_country))

# here, x_global needs to be from the same subset, otherwise the marketshare do not sum up to 1
delta_BNEF <- calculate_delta(projects_with_costs_in_interval, x_global$BNEF_interval)
delta_IRENA <- calculate_delta(irena_data, x_global$IRENA)

if(params$delta == "IRENA"){
  delta <- delta_IRENA
} else {
  delta <- delta_BNEF
}

```


#### Marketshare plots (`r params$delta`)

The market share of PV deplyoment in each currency and year is relevant. To calculate the market share of different currencies, we derive delta values from BNEF and IRENA datasets. Figure 1 visualises the market shares as a function of the currency.

```{r Marketshare plots comparison, fig.width = 11, fig.cap = "Comparison of capacity additions per year and currency area in BNEF and IRENA dataset", fig.width=fig.width.baseline, fig.asp=0.45, out.width=out.width.default}

bind_rows(delta_BNEF,delta_IRENA) %>% 
  get_delta_plot(plot_type = "absolute") +
  facet_grid(cols = vars(subset_name))

bind_rows(delta_BNEF,delta_IRENA) %>% 
  get_delta_plot(plot_type = "relative", legend = TRUE) +
  facet_grid(cols = vars(subset_name))


```

```{r Marketshare plots current, fig.width = 11, fig.cap = "Capacity additions per year and currency area", fig.width=fig.width.baseline, fig.asp=0.45, out.width=out.width.default}

get_delta_plot(delta)

```


### Step 2: Calculation of the formula variable

C are observed average cost in currency i. We use the variable average_local_costs

```{r Calculation of average_local_costs or C}

## C = average_local_costs: observed average cost per MW in currency i in year t (i.e. not a subset of all projects) 

average_local_costs <- projects_with_costs_in_interval %>% 
  group_by(year, local_currency) %>% 
  summarise(C = mean(local_value)) %>% ungroup() %>%
  rename(currency = local_currency) %>% 
  complete(year,currency, fill = list(C = 0))

```

The table shows which currency have average_local_costs in which year. 

### Step 3: Aggregation to weighted average global costs (P)

In the third step, we aggregate the data by yearly averages for each currency. The mean is weighted by the marketshare $\delta$ (in contrast to method 1). We calculate two different $P$: 

- $P$ for the unadjusted average global costs
- $P_{tilde}$ for the adjusted average global costs

```{r Calculation of the weighted average global cost P}

## w: exchange rate between currency i and the lead currency l in price notation
# e.g. "5 lead-dollars / 1 Euro" 

# I changed to simple variable name 

C <- average_local_costs

numeric_P_comp <- numeric(length(all_years) * length(relevant_currencies))

alpha <- tibble(
  "currency" = rep(relevant_currencies, length(all_years)),
  "year" = rep(all_years, length(relevant_currencies)),
  "alpha" = numeric_P_comp
)

P_comp <- tibble(
  "lead_currency" = lead_currency,
  "currency" = rep(relevant_currencies, length(all_years)),
  "year" = rep(all_years, length(relevant_currencies)),
  "nominal_costs" = numeric_P_comp
) %>% inner_join(defl, by = c("currency", "year"))

P_comp_tilde <- P_comp

for(t in all_years){
  
  for(i in relevant_currencies){

    delta_i_t <- filter(delta, year == t, currency == i) %>% pull(delta)
    w_i_t <- filter(exchange_rates_per_year, year == t, currency == i) %>% pull(rate)
    w_i_0 <- filter(exchange_rates_per_year, year == T0, currency == i) %>% pull(rate)
    C_i_t <- filter(C, year == t, currency == i) %>% pull(C)
    
    P_comp[P_comp$year == t & P_comp$currency == i, "nominal_costs"] <- delta_i_t * w_i_t * C_i_t
    P_comp_tilde[P_comp$year == t & P_comp$currency == i, "nominal_costs"] <- delta_i_t * w_i_0 * C_i_t
    

  }
  
}

# debug
alpha <- P_comp_tilde %>% 
  left_join(P_comp, by = c("currency", "year"), suffix = c("_tilde", "")) %>% 
  mutate(alpha = nominal_costs_tilde / nominal_costs) %>% 
  write_csv(paste0("output/currency_comparison2/alpha_", params$lead_currency,".csv"))

  
P <- calculate_P(P_comp)
P_tilde <- calculate_P(P_comp_tilde)


```


### Step 5: Calculation of the learning rate

A regression over $P_{t,2017}^l$ gives us the **un**adjusted global learning rates in `r lead_currency`,. The **un**adjusted rate includes both technological learning and changes in exchange rates. Therefore, it is different in different lead currencies. To get the same number in every lead currency (i.e. a truly global learning rate), we adjust the unadjusted global learning rate for the currency effect. Applying $\alpha$ to the learning rate calculation results in the adjusted global learning rate. The adjusted rate only includes technological learning. The corresponding learning curves are depicted in Figure 1.

```{r Calculation of unadjusted and adjusted learning rate}

corrected_learning_rates_list <- list()
corrected_learning_rates_data_list <- list()
corrected_learning_rates_rsquared_list <- list()

types <- c("unadjusted","adjusted")

for(type in types){
  corrected_learning_rates_list[[type]] <- list()
  corrected_learning_rates_data_list[[type]] <- list()
  corrected_learning_rates_rsquared_list[[type]] <- list()
  
  for(i in 1:length(intervals)){
    interval <- intervals[[i]]
   
    interval_years <- as.numeric(interval[1]):as.numeric(interval[2])
    interval_name <- names(intervals[i])
    
    if(type == "unadjusted"){
      relevant_P <- P$P_nominal
    } else {
      relevant_P <- P_tilde$P_nominal
    }
    
    regression_costs <- relevant_P %>% 
      filter(year %in% interval_years) %>% 
      mutate(costs2 = costs / as.double(relevant_P[relevant_P$year==2006,"costs"])) %>% 
      pull(costs2)
    
    regression_cumulative_capacity <- X %>% 
      filter(year %in% interval_years) %>% 
      pull(X)
    
    print(regression_costs)
    print(regression_cumulative_capacity)
    
    l <- calculate_learning_rate(regression_costs, regression_cumulative_capacity)
    
    print(l$b1)
    
    corrected_learning_rates_list[[type]][[interval_name]] <- l$learning_rate
    corrected_learning_rates_rsquared_list[[type]][[interval_name]] <- l$rsquared
    
    corrected_learning_rates_data_list[[type]][[interval_name]] <- data.frame(
      rsquared = l$rsquared,
      lead_currency = lead_currency,
      years = interval_years,
      capacity = regression_cumulative_capacity,
      costs = regression_costs,
      type = type,
      interval = interval_name
    )
  
  }
}
```

## Method 2: Results

We get the following learning rates using the corrected learning rates approach:

```{r Results table}

results[["corrected_learning_rates"]] <- corrected_learning_rates_list %>% 
  nested_list_to_matrix() %>% 
  matrix_to_tibble(names(intervals), types, "type")

results[["corrected_learning_rates_rsquared"]] <- corrected_learning_rates_rsquared_list %>% 
  nested_list_to_matrix() %>% 
  matrix_to_tibble(names(intervals), types, "type")

print_learning_rates_result(
  results[["corrected_learning_rates"]], results[["corrected_learning_rates_rsquared"]], 
  "type", paste("Global learning rates in lead currency", lead_currency))

```


```{r Plot of learning curves, fig.cap="Plot of the currency-corrected learning curve in 3 intervals", fig.width=fig.width.baseline * 0.8, fig.asp=0.6}

data_for_plots <- do.call("rbind", corrected_learning_rates_data_list$adjusted)

ggplot(data_for_plots, aes(capacity/1000, costs)) +
  geom_smooth(mapping = aes(col = interval, linetype = interval), method="lm", formula = (y ~ x), se=FALSE) +
  geom_point(data=. %>% filter(interval==combined_plot_interval), col = "black") +
  geom_text(data=. %>% filter(interval==combined_plot_interval), col = "black", aes(label=years), hjust=1.3, vjust=1, size=2.5, alpha = 0.8) +
  scale_x_continuous(trans="log", breaks = c(c(1,seq(0,10,2)) %o% 10^(0:4)), minor_breaks = seq(0,1,by = 0.5)) +
  scale_y_continuous(trans="log", breaks = c(2:10 %o% 10^(0:4))) +
  labs(x = "Cumulative capacity [GW]", 
       y = paste0("Weighted average costs [2017-", lead_currency, " / W]")) +
  guides(colour=guide_legend(title="Interval"),linetype=guide_legend(title="Interval")) +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1))
 
```

## Combined plot of simple and corrected learning rates

Comparison with the simple learning rates for every country

```{r Combined plot, fig.cap="Learning rates plot", fig.width=fig.width.baseline * 0.8, fig.asp=0.6}

combined_plot_data <- bind_rows(
    add_column(results$simple_learning_rates, type = "uncorrected"),
    add_column(results$corrected_learning_rates, currency = "USD")) %>% 
  filter(type != "unadjusted") %>%
  gather(key = "interval", value = "lr", contains("-")) %>% 
  mutate(
    type = fct_recode(type, "corrected" = "adjusted"),
    interval = fct_relevel(interval, names(intervals)), 
    legend = ifelse(type == "corrected", paste0(currency, " (", type, ")"), currency),
    legend = fct_relevel(legend, "USD (corrected)", after = Inf))



mypal = pal_npg("nrc")(length(relevant_currencies))

combined_plot_data %>% 
  ggplot(aes(fill = legend, y = lr, x = interval)) + 
    geom_bar(position="dodge", stat="identity") +
    scale_fill_manual(values = c(mypal,"#000000")) +
    labs(x = "", y = paste0("Learning rate")) +
    guides(fill=guide_legend(title="Currency"))    


```

```{r Save results}

r1 <- results$simple_learning_rates %>% 
  gather(starts_with('2'), key = interval, value = rate) 

r2 <- results$simple_learning_rates_rsquared %>% 
  gather(starts_with('2'), key = interval, value = rsquared)

lr_result_1 <- left_join(r1, r2, by = c("currency", "interval")) %>% 
    add_column(type = "uncorrected")


r1 <- results$corrected_learning_rates %>%
  filter(type == "adjusted") %>% 
  select(-type) %>% 
  gather(starts_with('2'), key = interval, value = rate) 

r2 <- results$corrected_learning_rates_rsquared %>%
  filter(type == "adjusted") %>% 
  select(-type) %>% 
  gather(starts_with('2'), key = interval, value = rsquared)

lr_result_2 <- left_join(r1, r2, by = c("interval")) %>% 
    add_column(type = "corrected", currency = params$lead_currency)
  
bind_rows(lr_result_1,lr_result_2) %>% 
  write_csv(paste0("output/learning_rate_results_",params$delta,"_",params$lead_currency,".csv"))


```


\clearpage

# Appendix

## Linear Regression and learning rate calculation

To derive the learning curves, we applied a single-factor learning curve as defined in [@Lindman:2012kv]:

$ln(C_t) = \beta_0 + \beta_1 \cdot ln(CC_t).$

with, $C_t$ as the cost at time $t$, $CC_t$ as cummulative capacity at time $t$, and $\beta_0$ and $\beta_1$ as linear regression estimates. The learning rates is then derived from $\beta_1$:

$LR = 1 - 2^{\beta_1}.$

Based on these formulas, we applied a linear regression model with OLS (Ordinary Least Squares) over the transformed data (with ln-ln transformation of the predictors, as visible in the first formula). 

# Literature
